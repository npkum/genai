<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p14" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_14{left:110px;bottom:739px;letter-spacing:0.1px;word-spacing:0.08px;}
#t2_14{left:524px;bottom:739px;}
#t3_14{left:550px;bottom:739px;letter-spacing:0.1px;word-spacing:0.05px;}
#t4_14{left:112px;bottom:683px;letter-spacing:0.1px;word-spacing:0.07px;}
#t5_14{left:473px;bottom:683px;letter-spacing:0.08px;word-spacing:0.07px;}
#t6_14{left:1063px;bottom:683px;letter-spacing:-0.17px;}
#t7_14{left:1137px;bottom:683px;letter-spacing:0.12px;word-spacing:0.07px;}
#t8_14{left:112px;bottom:661px;letter-spacing:0.1px;word-spacing:0.12px;}
#t9_14{left:112px;bottom:620px;letter-spacing:0.12px;word-spacing:0.06px;}
#ta_14{left:112px;bottom:598px;letter-spacing:0.13px;word-spacing:0.06px;}
#tb_14{left:847px;bottom:598px;letter-spacing:0.09px;}
#tc_14{left:112px;bottom:534px;letter-spacing:-0.09px;word-spacing:0.03px;}
#td_14{left:112px;bottom:504px;letter-spacing:-0.12px;}
#te_14{left:194px;bottom:504px;letter-spacing:-0.14px;}
#tf_14{left:242px;bottom:505px;letter-spacing:0.08px;word-spacing:-0.01px;}
#tg_14{left:551px;bottom:504px;letter-spacing:-0.11px;word-spacing:-0.04px;}
#th_14{left:678px;bottom:505px;letter-spacing:0.08px;word-spacing:0.01px;}
#ti_14{left:112px;bottom:478px;letter-spacing:0.11px;}
#tj_14{left:194px;bottom:478px;letter-spacing:0.04px;word-spacing:0.05px;}
#tk_14{left:194px;bottom:466px;letter-spacing:0.07px;word-spacing:-0.07px;}
#tl_14{left:551px;bottom:478px;letter-spacing:0.04px;word-spacing:0.03px;}
#tm_14{left:551px;bottom:466px;letter-spacing:0.08px;}
#tn_14{left:122px;bottom:433px;letter-spacing:-0.09px;}
#to_14{left:131px;bottom:421px;letter-spacing:0.04px;}
#tp_14{left:194px;bottom:433px;letter-spacing:0.04px;word-spacing:0.03px;}
#tq_14{left:194px;bottom:421px;letter-spacing:0.05px;}
#tr_14{left:551px;bottom:433px;letter-spacing:0.05px;}
#ts_14{left:115px;bottom:392px;letter-spacing:0.06px;}
#tt_14{left:127px;bottom:380px;letter-spacing:0.08px;}
#tu_14{left:194px;bottom:392px;letter-spacing:0.06px;word-spacing:0.01px;}
#tv_14{left:194px;bottom:380px;letter-spacing:0.06px;word-spacing:-0.02px;}
#tw_14{left:551px;bottom:392px;letter-spacing:0.07px;word-spacing:0.01px;}
#tx_14{left:114px;bottom:349px;letter-spacing:0.03px;}
#ty_14{left:126px;bottom:337px;letter-spacing:0.05px;word-spacing:0.02px;}
#tz_14{left:123px;bottom:325px;letter-spacing:0.04px;}
#t10_14{left:194px;bottom:349px;letter-spacing:0.05px;word-spacing:0.05px;}
#t11_14{left:194px;bottom:337px;letter-spacing:0.05px;word-spacing:0.08px;}
#t12_14{left:194px;bottom:325px;letter-spacing:0.01px;word-spacing:0.06px;}
#t13_14{left:551px;bottom:349px;letter-spacing:0.05px;word-spacing:0.05px;}
#t14_14{left:551px;bottom:337px;letter-spacing:0.03px;word-spacing:0.04px;}
#t15_14{left:128px;bottom:292px;letter-spacing:-0.12px;}
#t16_14{left:114px;bottom:281px;letter-spacing:0.24px;}
#t17_14{left:194px;bottom:292px;letter-spacing:0.04px;word-spacing:0.03px;}
#t18_14{left:194px;bottom:280px;letter-spacing:0.05px;word-spacing:0.02px;}
#t19_14{left:551px;bottom:292px;letter-spacing:0.07px;word-spacing:-0.06px;}
#t1a_14{left:551px;bottom:280px;letter-spacing:0.02px;word-spacing:0.05px;}
#t1b_14{left:115px;bottom:252px;letter-spacing:0.08px;}
#t1c_14{left:194px;bottom:252px;letter-spacing:0.05px;word-spacing:0.03px;}
#t1d_14{left:194px;bottom:239px;letter-spacing:0.06px;word-spacing:0.01px;}
#t1e_14{left:551px;bottom:252px;letter-spacing:0.05px;word-spacing:0.04px;}
#t1f_14{left:551px;bottom:239px;letter-spacing:0.06px;word-spacing:0.01px;}
#t1g_14{left:113px;bottom:213px;letter-spacing:0.06px;}
#t1h_14{left:194px;bottom:213px;letter-spacing:0.04px;word-spacing:0.04px;}
#t1i_14{left:194px;bottom:201px;letter-spacing:0.05px;word-spacing:0.02px;}
#t1j_14{left:551px;bottom:213px;letter-spacing:0.06px;word-spacing:0.01px;}
#t1k_14{left:551px;bottom:201px;letter-spacing:0.04px;word-spacing:0.06px;}
#t1l_14{left:936px;bottom:505px;letter-spacing:-0.06px;}
#t1m_14{left:1053px;bottom:505px;letter-spacing:-0.17px;}
#t1n_14{left:936px;bottom:479px;letter-spacing:0.09px;}
#t1o_14{left:1053px;bottom:479px;letter-spacing:0.06px;word-spacing:0.01px;}
#t1p_14{left:1053px;bottom:467px;letter-spacing:0.05px;word-spacing:0.05px;}
#t1q_14{left:1053px;bottom:454px;letter-spacing:0.06px;word-spacing:0.01px;}
#t1r_14{left:1053px;bottom:442px;letter-spacing:0.08px;word-spacing:-0.02px;}
#t1s_14{left:1053px;bottom:430px;letter-spacing:0.06px;word-spacing:0.01px;}
#t1t_14{left:936px;bottom:395px;letter-spacing:0.09px;word-spacing:0.16px;}
#t1u_14{left:1053px;bottom:395px;letter-spacing:0.06px;word-spacing:0.01px;}
#t1v_14{left:1053px;bottom:382px;letter-spacing:0.06px;word-spacing:0.03px;}
#t1w_14{left:1053px;bottom:370px;letter-spacing:0.08px;word-spacing:-0.01px;}
#t1x_14{left:1053px;bottom:358px;letter-spacing:0.06px;word-spacing:-0.01px;}
#t1y_14{left:1239px;bottom:358px;letter-spacing:0.14px;word-spacing:-0.05px;}
#t1z_14{left:1301px;bottom:358px;letter-spacing:0.09px;}
#t20_14{left:1053px;bottom:345px;letter-spacing:0.08px;word-spacing:-0.04px;}
#t21_14{left:936px;bottom:310px;letter-spacing:0.15px;}
#t22_14{left:1053px;bottom:310px;letter-spacing:0.05px;word-spacing:0.02px;}
#t23_14{left:1053px;bottom:298px;letter-spacing:0.05px;word-spacing:0.06px;}
#t24_14{left:1053px;bottom:286px;letter-spacing:0.08px;word-spacing:-0.07px;}
#t25_14{left:936px;bottom:255px;letter-spacing:0.11px;}
#t26_14{left:1053px;bottom:255px;letter-spacing:0.07px;word-spacing:-0.01px;}
#t27_14{left:1053px;bottom:243px;letter-spacing:0.06px;word-spacing:0.02px;}
#t28_14{left:1053px;bottom:231px;letter-spacing:0.05px;word-spacing:-0.01px;}
#t29_14{left:1053px;bottom:218px;letter-spacing:0.04px;word-spacing:0.07px;}
#t2a_14{left:1053px;bottom:206px;letter-spacing:0.07px;}
#t2b_14{left:939px;bottom:533px;letter-spacing:-0.1px;word-spacing:0.01px;}
#t2c_14{left:1268px;bottom:782px;letter-spacing:-0.09px;}
#t2d_14{left:1194px;bottom:762px;letter-spacing:-0.1px;word-spacing:0.11px;}
#t2e_14{left:99px;bottom:24px;letter-spacing:0.1px;}

.s0_14{font-size:25px;font-family:DejaVuSans_7l;color:#000;}
.s1_14{font-size:18px;font-family:Carlito_7-;color:#000;}
.s2_14{font-size:18px;font-family:Carlito_7-;color:#1F1F1F;}
.s3_14{font-size:18px;font-family:Carlito_7-;color:#333;}
.s4_14{font-size:17px;font-family:Carlito-Bold_8j;color:#333;}
.s5_14{font-size:14px;font-family:Carlito-Bold_8j;color:#000;}
.s6_14{font-size:12px;font-family:Carlito-Bold_8j;color:#000;}
.s7_14{font-size:12px;font-family:Carlito_7-;color:#000;}
.s8_14{font-size:11px;font-family:Carlito_7-;color:#000;}
.s9_14{font-size:12px;font-family:Carlito_7-;color:#0563C1;}
.sa_14{font-size:17px;font-family:Carlito-Bold_8j;color:#000;}
.sb_14{font-size:15px;font-family:Carlito_7-;color:#0563C1;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts14" type="text/css" >

@font-face {
	font-family: Carlito-Bold_8j;
	src: url("fonts/Carlito-Bold_8j.woff") format("woff");
}

@font-face {
	font-family: Carlito_7-;
	src: url("fonts/Carlito_7-.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_7l;
	src: url("fonts/DejaVuSans_7l.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg14Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg14" style="-webkit-user-select: none;"><object width="1466" height="825" data="14/14.svg" type="image/svg+xml" id="pdf14" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_14" class="t s0_14">Generative AI Project Life Cycle </span><span id="t2_14" class="t s0_14">- </span><span id="t3_14" class="t s0_14">Choose existing model or pre-train </span>
<span id="t4_14" class="t s1_14" data-mappings='[[38,"ti"]]'>In general, you might tend to use exisng LLM , t </span><span id="t5_14" class="t s2_14" data-mappings='[[23,"ti"]]'>his saves you a lot of me and can get you to a working prototype much faster. </span><span id="t6_14" class="t s1_14">However, </span><span id="t7_14" class="t s3_14" data-mappings='[[20,"ti"]]'>there could be situaon where you </span>
<span id="t8_14" class="t s3_14" data-mappings='[[4,"fi"],[111,"ti"]]'>may ﬁnd it necessary to pretrain your own model from scratch to achieve good task performance for domain adaptaon. </span>
<span id="t9_14" class="t s3_14" data-mappings='[[60,"ti"],[133,"fi"],[142,"ti"]]'>If your domain requires highly accurate results within a parcular domain unlike generalised LLM you may need to perform domain speciﬁc adaptaon to achieve good </span>
<span id="ta_14" class="t s3_14" data-mappings='[[85,"ti"]]'>performance. Example of these domains are Legal, Medical, Finance, Climate, Pharmaceucal and </span><span id="tb_14" class="t s3_14" data-mappings='[[5,"ti"]]'>Educaon </span>
<span id="tc_14" class="t s4_14" data-mappings='[[43,"fi"]]'>Comparison of general LLMs and domain-speciﬁc LLMs </span>
<span id="td_14" class="t s5_14">Criteria </span><span id="te_14" class="t s5_14">General </span><span id="tf_14" class="t s6_14">LLMs (e.g. Chat GPT 3.5, PaLM, Chohere, Falcon etc) </span><span id="tg_14" class="t s5_14" data-mappings='[[12,"fi"]]'>Domain-speciﬁc LLMs </span><span id="th_14" class="t s6_14">(e.g. FoodUDT-1B) </span>
<span id="ti_14" class="t s7_14">Purpose </span><span id="tj_14" class="t s7_14">Developed to understand and generate text across a broad range of </span>
<span id="tk_14" class="t s7_14">topics and contexts. </span>
<span id="tl_14" class="t s7_14" data-mappings='[[5,"fi"],[60,"ti"]]'>Speciﬁcally trained to understand and generate text in a parcular </span>
<span id="tm_14" class="t s7_14">niche or domain. </span>
<span id="tn_14" class="t s7_14">Training </span>
<span id="to_14" class="t s7_14">data </span>
<span id="tp_14" class="t s7_14">Trained on diverse internet text, encompassing a wide range of </span>
<span id="tq_14" class="t s7_14">topics </span>
<span id="tr_14" class="t s7_14" data-mappings='[[23,"fi"]]'>Trained on domain-speciﬁc data (~ 500K recipes). </span>
<span id="ts_14" class="t s7_14">Knowledge </span>
<span id="tt_14" class="t s7_14">depth </span>
<span id="tu_14" class="t s7_14">General understanding of a wide range of topics, including domain- </span>
<span id="tv_14" class="t s7_14" data-mappings='[[5,"fi"]]'>speciﬁc topics at a high level. </span>
<span id="tw_14" class="t s7_14" data-mappings='[[27,"fi"]]'>Deep understanding of speciﬁc domain. </span>
<span id="tx_14" class="t s7_14">Representa </span>
<span id="ty_14" class="t s7_14" data-mappings='[[0,"ti"]]'>on of </span>
<span id="tz_14" class="t s7_14">context </span>
<span id="t10_14" class="t s7_14">Represents text and context based on a broad understanding of </span>
<span id="t11_14" class="t s7_14">language and world knowledge. For example, “apple” is closer to </span>
<span id="t12_14" class="t s7_14">“iPhone” than to “apple pie”. </span>
<span id="t13_14" class="t s7_14">Represents text and context based on deep, specialized knowledge. </span>
<span id="t14_14" class="t s7_14">For example, “apple” is closer to “apple pie” than to “iPhone.” </span>
<span id="t15_14" class="t s7_14">Word </span>
<span id="t16_14" class="t s8_14" data-mappings='[[7,"ti"]]'>associaons </span>
<span id="t17_14" class="t s7_14" data-mappings='[[13,"ti"]]'>Forms associaons based on generic context. For example “fruit”, </span>
<span id="t18_14" class="t s7_14">“sugar” and “dessert” are equally related. </span>
<span id="t19_14" class="t s7_14" data-mappings='[[13,"ti"],[39,"fi"]]'>Forms associaons based on domain-speciﬁc context. For example, </span>
<span id="t1a_14" class="t s7_14">“dessert” and “sugar” are more related than “fruit” and “sugar.” </span>
<span id="t1b_14" class="t s7_14" data-mappings='[[6,"ti"]]'>Limitaons </span><span id="t1c_14" class="t s7_14">May not possess in-depth understanding or generate accurate </span>
<span id="t1d_14" class="t s7_14">outputs in specialized domains. </span>
<span id="t1e_14" class="t s7_14" data-mappings='[[20,"fi"]]'>Limited to its speciﬁc domain and may not generate accurate outputs </span>
<span id="t1f_14" class="t s7_14">outside that domain. </span>
<span id="t1g_14" class="t s7_14">Advantages </span><span id="t1h_14" class="t s7_14" data-mappings='[[5,"ti"],[28,"ti"],[44,"ti"],[56,"ti"]]'>Versale in general conversaonal, summarisaon, translaon and </span>
<span id="t1i_14" class="t s7_14">other generic tasks. </span>
<span id="t1j_14" class="t s7_14" data-mappings='[[21,"ffi"],[40,"fi"]]'>Highly accurate and eﬃcient in its speciﬁc domain due to target </span>
<span id="t1k_14" class="t s7_14">training ( recipe, food and cooking) </span>
<span id="t1l_14" class="t s5_14">LLM </span><span id="t1m_14" class="t s5_14">Details </span>
<span id="t1n_14" class="t s9_14">BloombergGPT </span><span id="t1o_14" class="t s7_14">is a causal language model designed with decoder-only architecture. </span>
<span id="t1p_14" class="t s7_14">The model operated with 50 billion parameters and was trained from </span>
<span id="t1q_14" class="t s7_14" data-mappings='[[25,"fi"],[36,"fi"]]'>scratch with domain speciﬁc data in ﬁnance. It outperformed similar </span>
<span id="t1r_14" class="t s7_14" data-mappings='[[10,"fi"],[35,"fi"]]'>models on ﬁnancial tasks by a signiﬁcant margin while maintaining or </span>
<span id="t1s_14" class="t s7_14" data-mappings='[[2,"tt"]]'>beering the others on general language tasks. </span>
<span id="t1t_14" class="t s9_14">Med-PaLM 2 </span><span id="t1u_14" class="t s7_14">is a custom language model that Google built by training on curated </span>
<span id="t1v_14" class="t s7_14" data-mappings='[[62,"ti"]]'>medical datasets. The model can accurately answer medical quesons, </span>
<span id="t1w_14" class="t s7_14" data-mappings='[[2,"tti"]]'>pung it on par with medical professionals in some use cases. When </span>
<span id="t1x_14" class="t s7_14">put to the test, MedPalm 2 scored an </span><span id="t1y_14" class="t s9_14">86.5% mark </span><span id="t1z_14" class="t s7_14">on the MedQA </span>
<span id="t20_14" class="t s7_14" data-mappings='[[14,"ti"],[49,"ti"],[57,"ti"]]'>dataset consisng of US Medical Licensing Examinaon quesons. </span>
<span id="t21_14" class="t s9_14">KAI-GPT </span><span id="t22_14" class="t s7_14" data-mappings='[[53,"ti"]]'>is a large language model trained to deliver conversaonal AI in the </span>
<span id="t23_14" class="t s7_14">banking industry. The model enables transparent, safe, and accurate use </span>
<span id="t24_14" class="t s7_14" data-mappings='[[9,"ti"]]'>of generave AI models when servicing banking customers. </span>
<span id="t25_14" class="t s9_14">FinGPT </span><span id="t26_14" class="t s7_14" data-mappings='[[49,"fi"]]'>is a lightweight language model pre-trained with ﬁnancial data. It </span>
<span id="t27_14" class="t s7_14" data-mappings='[[17,"ff"],[37,"ti"]]'>provides a more aﬀordable training opon than the proprietary </span>
<span id="t28_14" class="t s7_14">BloombergGPT. It also incorporates reinforcement learning from human </span>
<span id="t29_14" class="t s7_14" data-mappings='[[38,"ti"]]'>feedback to enable further personalizaon. FinGPT scores well against </span>
<span id="t2a_14" class="t s7_14" data-mappings='[[16,"fi"],[28,"ti"]]'>other models on ﬁnancial senment analysis datasets. </span>
<span id="t2b_14" class="t s4_14" data-mappings='[[28,"fi"]]'>Few examples of Domain-speciﬁc LLMs </span>
<span id="t2c_14" class="t sa_14">Select </span>
<span id="t2d_14" class="t sa_14" data-mappings='[[4,"ti"]]'>Exisng models or pre-train </span>
<span id="t2e_14" class="t sb_14">Kumar </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
