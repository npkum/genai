<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p20" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_20{left:112px;bottom:740px;letter-spacing:0.01px;word-spacing:0.08px;}
#t2_20{left:736px;bottom:740px;letter-spacing:-0.16px;word-spacing:0.32px;}
#t3_20{left:866px;bottom:745px;letter-spacing:-0.37px;word-spacing:0.16px;}
#t4_20{left:123px;bottom:708px;letter-spacing:0.13px;word-spacing:0.05px;}
#t5_20{left:124px;bottom:677px;letter-spacing:0.08px;word-spacing:0.09px;}
#t6_20{left:126px;bottom:648px;letter-spacing:-0.17px;word-spacing:0.07px;}
#t7_20{left:155px;bottom:635px;letter-spacing:-0.19px;}
#t8_20{left:277px;bottom:648px;letter-spacing:-0.12px;}
#t9_20{left:277px;bottom:635px;letter-spacing:-0.15px;word-spacing:0.04px;}
#ta_20{left:141px;bottom:586px;letter-spacing:-0.18px;word-spacing:0.02px;}
#tb_20{left:122px;bottom:572px;letter-spacing:-0.11px;word-spacing:-0.04px;}
#tc_20{left:152px;bottom:558px;letter-spacing:-0.1px;word-spacing:-0.06px;}
#td_20{left:277px;bottom:586px;letter-spacing:-0.13px;word-spacing:0.02px;}
#te_20{left:277px;bottom:572px;letter-spacing:-0.16px;word-spacing:0.08px;}
#tf_20{left:1069px;bottom:606px;letter-spacing:-0.31px;}
#tg_20{left:1162px;bottom:645px;letter-spacing:-0.18px;}
#th_20{left:1176px;bottom:633px;letter-spacing:-0.17px;}
#ti_20{left:1155px;bottom:605px;letter-spacing:-0.21px;word-spacing:0.09px;}
#tj_20{left:1168px;bottom:573px;letter-spacing:-0.15px;word-spacing:-0.13px;}
#tk_20{left:1266px;bottom:606px;letter-spacing:-0.11px;}
#tl_20{left:1266px;bottom:572px;letter-spacing:-0.11px;}
#tm_20{left:1056px;bottom:638px;letter-spacing:-0.12px;word-spacing:0.05px;}
#tn_20{left:1340px;bottom:667px;letter-spacing:-0.15px;}
#to_20{left:1351px;bottom:658px;letter-spacing:-0.14px;}
#tp_20{left:1343px;bottom:613px;letter-spacing:-0.19px;}
#tq_20{left:1351px;bottom:604px;letter-spacing:-0.14px;}
#tr_20{left:1354px;bottom:559px;letter-spacing:-0.18px;}
#ts_20{left:1352px;bottom:549px;letter-spacing:-0.14px;}
#tt_20{left:1265px;bottom:641px;letter-spacing:-0.11px;}
#tu_20{left:147px;bottom:501px;letter-spacing:-0.2px;word-spacing:0.05px;}
#tv_20{left:277px;bottom:501px;letter-spacing:-0.15px;word-spacing:0.07px;}
#tw_20{left:277px;bottom:487px;letter-spacing:-0.15px;word-spacing:0.05px;}
#tx_20{left:1091px;bottom:438px;letter-spacing:-0.2px;word-spacing:0.07px;}
#ty_20{left:1220px;bottom:439px;letter-spacing:-0.19px;word-spacing:0.09px;}
#tz_20{left:1238px;bottom:476px;letter-spacing:-0.18px;word-spacing:0.05px;}
#t10_20{left:1086px;bottom:489px;letter-spacing:-0.24px;word-spacing:0.14px;}
#t11_20{left:1160px;bottom:510px;letter-spacing:-0.2px;word-spacing:-0.07px;}
#t12_20{left:124px;bottom:394px;letter-spacing:0.11px;word-spacing:0.07px;}
#t13_20{left:163px;bottom:373px;letter-spacing:-0.15px;}
#t14_20{left:277px;bottom:373px;letter-spacing:-0.13px;word-spacing:0.02px;}
#t15_20{left:277px;bottom:359px;letter-spacing:-0.13px;word-spacing:0.04px;}
#t16_20{left:1070px;bottom:393px;letter-spacing:0.07px;}
#t17_20{left:1162px;bottom:395px;letter-spacing:0.03px;}
#t18_20{left:1307px;bottom:397px;letter-spacing:0.1px;}
#t19_20{left:1049px;bottom:370px;letter-spacing:-0.07px;word-spacing:0.01px;}
#t1a_20{left:1049px;bottom:358px;letter-spacing:-0.06px;word-spacing:-0.07px;}
#t1b_20{left:1049px;bottom:346px;letter-spacing:-0.1px;word-spacing:0.11px;}
#t1c_20{left:132px;bottom:329px;letter-spacing:-0.18px;}
#t1d_20{left:277px;bottom:329px;letter-spacing:-0.14px;word-spacing:0.04px;}
#t1e_20{left:277px;bottom:315px;letter-spacing:-0.13px;word-spacing:0.01px;}
#t1f_20{left:738px;bottom:315px;letter-spacing:-0.17px;}
#t1g_20{left:770px;bottom:315px;letter-spacing:-0.14px;word-spacing:0.02px;}
#t1h_20{left:277px;bottom:301px;letter-spacing:-0.14px;word-spacing:0.03px;}
#t1i_20{left:1171px;bottom:372px;letter-spacing:-0.13px;}
#t1j_20{left:1171px;bottom:361px;letter-spacing:-0.09px;word-spacing:0.09px;}
#t1k_20{left:1171px;bottom:349px;letter-spacing:-0.08px;word-spacing:-0.05px;}
#t1l_20{left:1171px;bottom:337px;letter-spacing:-0.11px;}
#t1m_20{left:1179px;bottom:320px;letter-spacing:0.12px;}
#t1n_20{left:164px;bottom:268px;letter-spacing:-0.17px;}
#t1o_20{left:277px;bottom:268px;letter-spacing:-0.13px;word-spacing:0.03px;}
#t1p_20{left:277px;bottom:254px;letter-spacing:-0.14px;}
#t1q_20{left:1290px;bottom:370px;letter-spacing:-0.08px;word-spacing:-0.05px;}
#t1r_20{left:1290px;bottom:358px;letter-spacing:-0.05px;word-spacing:-0.07px;}
#t1s_20{left:1290px;bottom:347px;letter-spacing:-0.09px;word-spacing:0.1px;}
#t1t_20{left:1290px;bottom:335px;letter-spacing:-0.08px;}
#t1u_20{left:1289px;bottom:337px;letter-spacing:-0.21px;}
#t1v_20{left:1286px;bottom:315px;letter-spacing:-0.16px;word-spacing:-0.1px;}
#t1w_20{left:1344px;bottom:317px;letter-spacing:-0.06px;word-spacing:0.1px;}
#t1x_20{left:122px;bottom:208px;letter-spacing:0.1px;word-spacing:0.04px;}
#t1y_20{left:1047px;bottom:52px;letter-spacing:0.18px;word-spacing:-0.1px;}
#t1z_20{left:1076px;bottom:52px;letter-spacing:0.18px;word-spacing:-0.03px;}
#t20_20{left:138px;bottom:183px;letter-spacing:-0.13px;word-spacing:0.02px;}
#t21_20{left:132px;bottom:160px;letter-spacing:-0.13px;word-spacing:0.03px;}
#t22_20{left:132px;bottom:144px;letter-spacing:-0.14px;word-spacing:0.05px;}
#t23_20{left:277px;bottom:458px;}
#t24_20{left:298px;bottom:457px;letter-spacing:-0.15px;word-spacing:0.05px;}
#t25_20{left:277px;bottom:441px;}
#t26_20{left:298px;bottom:440px;letter-spacing:-0.14px;word-spacing:0.03px;}
#t27_20{left:277px;bottom:425px;}
#t28_20{left:298px;bottom:424px;letter-spacing:-0.15px;word-spacing:0.06px;}
#t29_20{left:133px;bottom:128px;}
#t2a_20{left:154px;bottom:127px;letter-spacing:-0.16px;word-spacing:0.1px;}
#t2b_20{left:134px;bottom:89px;}
#t2c_20{left:154px;bottom:88px;letter-spacing:-0.15px;word-spacing:0.05px;}
#t2d_20{left:134px;bottom:109px;}
#t2e_20{left:154px;bottom:109px;letter-spacing:-0.14px;word-spacing:0.05px;}
#t2f_20{left:1198px;bottom:782px;letter-spacing:-0.09px;word-spacing:0.11px;}
#t2g_20{left:1261px;bottom:762px;letter-spacing:-0.08px;}
#t2h_20{left:99px;bottom:24px;letter-spacing:0.1px;}

.s0_20{font-size:38px;font-family:DejaVuSans_7l;color:#000;}
.s1_20{font-size:28px;font-family:DejaVuSans_7l;color:#000;}
.s2_20{font-size:18px;font-family:Carlito_7-;color:#000;}
.s3_20{font-size:15px;font-family:Carlito-Bold_8j;color:#000;}
.s4_20{font-size:14px;font-family:Carlito-Bold_8j;color:#000;}
.s5_20{font-size:14px;font-family:Carlito_7-;color:#000;}
.s6_20{font-size:11px;font-family:Carlito_7-;color:#000;}
.s7_20{font-size:10px;font-family:Carlito_7-;color:#000;}
.s8_20{font-size:8px;font-family:Carlito_7-;color:#000;}
.s9_20{font-size:12px;font-family:Carlito_7-;color:#000;}
.sa_20{font-size:10px;font-family:DejaVuSans_7l;color:#000;}
.sb_20{font-size:12px;font-family:Carlito-Bold_8j;color:#000;}
.sc_20{font-size:6px;font-family:Carlito_7-;color:#000;}
.sd_20{font-size:8px;font-family:Carlito-Italic_84;color:#000;}
.se_20{font-size:8px;font-family:Carlito-Italic_84;color:#0563C1;}
.sf_20{font-size:14px;font-family:LiberationSans_7g;color:#000;}
.sg_20{font-size:17px;font-family:Carlito-Bold_8j;color:#000;}
.sh_20{font-size:15px;font-family:Carlito_7-;color:#0563C1;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts20" type="text/css" >

@font-face {
	font-family: Carlito-Bold_8j;
	src: url("fonts/Carlito-Bold_8j.woff") format("woff");
}

@font-face {
	font-family: Carlito-Italic_84;
	src: url("fonts/Carlito-Italic_84.woff") format("woff");
}

@font-face {
	font-family: Carlito_7-;
	src: url("fonts/Carlito_7-.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_7l;
	src: url("fonts/DejaVuSans_7l.woff") format("woff");
}

@font-face {
	font-family: LiberationSans_7g;
	src: url("fonts/LiberationSans_7g.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg20Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg20" style="-webkit-user-select: none;"><object width="1466" height="825" data="20/20.svg" type="image/svg+xml" id="pdf20" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_20" class="t s0_20">Generative AI Project Life Cycle </span><span id="t2_20" class="t s0_20">- PEFT </span><span id="t3_20" class="t s1_20" data-mappings='[[12,"ffi"]]'>(Parameter Eﬃcient Fine-tunning) </span>
<span id="t4_20" class="t s2_20" data-mappings='[[8,"fi"]]'>In full ﬁne-tuning every model weight is updated during supervised learning, whereas in PEFT technique only updates a small subset of parameters. </span>
<span id="t5_20" class="t s3_20">Overview of PEFT </span>
<span id="t6_20" class="t s4_20">Train small number of </span>
<span id="t7_20" class="t s4_20">parameters </span>
<span id="t8_20" class="t s5_20" data-mappings='[[16,"ffi"],[23,"fi"]]'>With parameter eﬃcient ﬁne-tuning, you train only a small number of weights, which results in a much smaller footprint overall, as </span>
<span id="t9_20" class="t s5_20">small as megabytes depending on the task. </span>
<span id="ta_20" class="t s4_20">New parameters </span>
<span id="tb_20" class="t s4_20">combined with original </span>
<span id="tc_20" class="t s4_20">LLM weights </span>
<span id="td_20" class="t s5_20" data-mappings='[[87,"ffi"],[100,"ti"]]'>The new parameters are combined with the original LLM weights for inference, allowing eﬃcient adaptaon of the original model </span>
<span id="te_20" class="t s5_20" data-mappings='[[6,"ti"]]'>to mulple tasks. </span>
<span id="tf_20" class="t s6_20">LLM </span>
<span id="tg_20" class="t s6_20">Summarise </span>
<span id="th_20" class="t s6_20">PEFT </span>
<span id="ti_20" class="t s6_20">Generate PEFT </span>
<span id="tj_20" class="t s6_20">QA PEFT </span>
<span id="tk_20" class="t s7_20">MBs </span>
<span id="tl_20" class="t s7_20">MBs </span>
<span id="tm_20" class="t s7_20">size : GBs </span>
<span id="tn_20" class="t s8_20">Summarise </span>
<span id="to_20" class="t s8_20">LLM </span>
<span id="tp_20" class="t s8_20">Generate </span>
<span id="tq_20" class="t s8_20">LLM </span>
<span id="tr_20" class="t s8_20">QA </span>
<span id="ts_20" class="t s8_20">LLM </span>
<span id="tt_20" class="t s7_20">MBs </span>
<span id="tu_20" class="t s4_20" data-mappings='[[12,"ff"]]'>PEFT Trade-oﬀ </span><span id="tv_20" class="t s5_20" data-mappings='[[53,"ffi"],[60,"fi"],[89,"ff"],[107,"ffi"]]'>There are several methods you can use for parameter eﬃcient ﬁne-tuning, each with trade-oﬀs on: parameter eﬃciency, memory </span>
<span id="tw_20" class="t s5_20" data-mappings='[[1,"ffi"]]'>eﬃciency, training speed, model quality and inference costs. </span>
<span id="tx_20" class="t s6_20">Model Performance </span><span id="ty_20" class="t s6_20">Inference Costs </span>
<span id="tz_20" class="t s6_20" data-mappings='[[8,"ffi"]]'>Memory Eﬃciency </span>
<span id="t10_20" class="t s6_20">Training Speed </span>
<span id="t11_20" class="t s6_20" data-mappings='[[11,"ffi"]]'>Parameter Eﬃciency </span>
<span id="t12_20" class="t s3_20">PEFT methods </span>
<span id="t13_20" class="t s4_20" data-mappings='[[5,"ti"]]'>Selecve </span><span id="t14_20" class="t s5_20" data-mappings='[[5,"ti"],[32,"fi"]]'>Selecve methods are those that ﬁne-tune only a subset of the original LLM parameters. Some of the approaches are: </span>
<span id="t15_20" class="t s5_20" data-mappings='[[49,"fi"]]'>train only certain components of the model, speciﬁc layers, individual parameter </span>
<span id="t16_20" class="t s9_20" data-mappings='[[5,"ti"]]'>Selecve </span>
<span id="t17_20" class="t s9_20" data-mappings='[[14,"ti"]]'>Reparameterizaon </span><span id="t18_20" class="t s9_20" data-mappings='[[4,"ti"]]'>Addive </span>
<span id="t19_20" class="t sa_20">Subset of initial </span>
<span id="t1a_20" class="t sa_20">LLM parameters </span>
<span id="t1b_20" class="t sa_20" data-mappings='[[3,"fi"]]'>to ﬁne-tune </span>
<span id="t1c_20" class="t s4_20" data-mappings='[[14,"ti"]]'>Reparameterizaon </span><span id="t1d_20" class="t s5_20" data-mappings='[[109,"ti"]]'>This methods also works with the original LLM parameters but reduce the number of parameters to train by creang new low rank </span>
<span id="t1e_20" class="t s5_20" data-mappings='[[10,"ti"]]'>transformaons of the original weights. A commonly used technique of this type is </span><span id="t1f_20" class="t s4_20">LoRA </span><span id="t1g_20" class="t s5_20" data-mappings='[[16,"ti"]]'>(Low-rank Adaptaon). LoRA is a strategy </span>
<span id="t1h_20" class="t s5_20" data-mappings='[[59,"fi"]]'>that reduces the number of parameters to be trained during ﬁne-tuning by freezing all of the original model parameters. </span>
<span id="t1i_20" class="t sa_20">Reparametrize </span>
<span id="t1j_20" class="t sa_20">model weights </span>
<span id="t1k_20" class="t sa_20">using low-rank </span>
<span id="t1l_20" class="t sa_20">representation </span>
<span id="t1m_20" class="t sb_20">LoRA </span>
<span id="t1n_20" class="t s4_20" data-mappings='[[4,"ti"]]'>Addive </span><span id="t1o_20" class="t s5_20" data-mappings='[[4,"ti"],[26,"fi"]]'>Addive methods carry out ﬁne-tuning by keeping all of the original LLM weights frozen and introducing new trainable </span>
<span id="t1p_20" class="t s5_20" data-mappings='[[52,"ft"]]'>components. Two main approaches are : Adapter and Soﬅ prompt. </span>
<span id="t1q_20" class="t sa_20">Add trainable </span>
<span id="t1r_20" class="t sa_20">layers or </span>
<span id="t1s_20" class="t sa_20">parameters to </span>
<span id="t1t_20" class="t sa_20">LLM </span>
<span id="t1u_20" class="t s6_20">Adapters </span>
<span id="t1v_20" class="t s6_20" data-mappings='[[2,"ft"]]'>Soﬅ Prompts </span><span id="t1w_20" class="t sc_20">Prompt Tunning </span>
<span id="t1x_20" class="t s3_20" data-mappings='[[57,"ti"]]'>How to choose LoRA rank and how does it impact LLM evaluaon metrics ? </span>
<span id="t1y_20" class="t sd_20">Source : </span><span id="t1z_20" class="t se_20" data-mappings='[[57,"ft"],[66,"ti"]]'>LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS - by Microsoﬅ Corporaon </span>
<span id="t20_20" class="t s5_20">In principle, the smaller the rank, the smaller the number of trainable parameters, and the bigger the savings on compute. </span>
<span id="t21_20" class="t s5_20" data-mappings='[[50,"ft"],[67,"ff"]]'>The paper LoRA published by researchers at Microsoﬅ explored how diﬀerent choices of rank impact the LLM model performance. The summary of results </span>
<span id="t22_20" class="t s5_20">in this table are: </span>
<span id="t23_20" class="t sf_20">• </span><span id="t24_20" class="t s5_20" data-mappings='[[86,"ti"]]'>values highlighted in bold indicate the best scores that were achieved for each evaluaon metric. </span>
<span id="t25_20" class="t sf_20">• </span><span id="t26_20" class="t s5_20" data-mappings='[[17,"ti"]]'>plateau in validaon loss for rank greater than 16 i.e. using larger LoRA matrices didn’t improve the performance/accuracy score. </span>
<span id="t27_20" class="t sf_20">• </span><span id="t28_20" class="t s5_20" data-mappings='[[56,"ff"]]'>ranks in the range of 4 to 32 can provide a good trade-oﬀ between reducing trainable parameters and preserving performance. </span>
<span id="t29_20" class="t sf_20">• </span><span id="t2a_20" class="t s5_20" data-mappings='[[86,"ti"]]'>values highlighted in bold indicate the best scores that were achieved for each evaluaon metric. </span>
<span id="t2b_20" class="t sf_20">• </span><span id="t2c_20" class="t s5_20" data-mappings='[[56,"ff"]]'>ranks in the range of 4 to 32 can provide a good trade-oﬀ between reducing trainable parameters and preserving performance. </span>
<span id="t2d_20" class="t sf_20">• </span><span id="t2e_20" class="t s5_20" data-mappings='[[17,"ti"]]'>plateau in validaon loss for rank greater than 16 i.e. using larger LoRA matrices didn’t improve the performance/accuracy score. </span>
<span id="t2f_20" class="t sg_20">Adapt and align model </span>
<span id="t2g_20" class="t sg_20">PEFT </span>
<span id="t2h_20" class="t sh_20">Kumar </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
