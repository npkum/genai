<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p23" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_23{left:112px;bottom:735px;letter-spacing:-0.05px;word-spacing:0.12px;}
#t2_23{left:619px;bottom:735px;letter-spacing:-0.07px;word-spacing:0.08px;}
#t3_23{left:76px;bottom:36px;letter-spacing:0.13px;}
#t4_23{left:1186px;bottom:781px;letter-spacing:-0.09px;word-spacing:0.05px;}
#t5_23{left:1170px;bottom:761px;letter-spacing:-0.09px;}
#t6_23{left:91px;bottom:694px;letter-spacing:0.12px;word-spacing:0.09px;}
#t7_23{left:91px;bottom:675px;}
#t8_23{left:112px;bottom:672px;letter-spacing:0.1px;word-spacing:0.09px;}
#t9_23{left:105px;bottom:436px;}
#ta_23{left:143px;bottom:435px;}
#tb_23{left:164px;bottom:436px;letter-spacing:-0.14px;word-spacing:0.06px;}
#tc_23{left:143px;bottom:421px;}
#td_23{left:164px;bottom:422px;letter-spacing:-0.14px;word-spacing:0.04px;}
#te_23{left:1233px;bottom:408px;letter-spacing:-0.05px;}
#tf_23{left:1240px;bottom:397px;letter-spacing:0.05px;}
#tg_23{left:105px;bottom:393px;}
#th_23{left:143px;bottom:391px;}
#ti_23{left:164px;bottom:393px;letter-spacing:-0.14px;word-spacing:0.02px;}
#tj_23{left:1066px;bottom:409px;letter-spacing:-0.12px;}
#tk_23{left:1066px;bottom:397px;letter-spacing:-0.11px;}
#tl_23{left:1128px;bottom:465px;letter-spacing:0.08px;word-spacing:-0.08px;}
#tm_23{left:1306px;bottom:465px;letter-spacing:0.06px;word-spacing:-0.06px;}
#tn_23{left:105px;bottom:347px;}
#to_23{left:143px;bottom:346px;}
#tp_23{left:164px;bottom:347px;letter-spacing:-0.15px;word-spacing:0.08px;}
#tq_23{left:483px;bottom:347px;letter-spacing:-0.13px;word-spacing:0.02px;}
#tr_23{left:164px;bottom:333px;letter-spacing:-0.17px;}
#ts_23{left:143px;bottom:318px;}
#tt_23{left:164px;bottom:320px;letter-spacing:-0.14px;word-spacing:0.02px;}
#tu_23{left:90px;bottom:459px;letter-spacing:0.09px;word-spacing:0.06px;}
#tv_23{left:105px;bottom:285px;}
#tw_23{left:143px;bottom:283px;}
#tx_23{left:164px;bottom:285px;letter-spacing:-0.14px;word-spacing:0.04px;}
#ty_23{left:143px;bottom:270px;}
#tz_23{left:164px;bottom:271px;letter-spacing:-0.14px;word-spacing:0.02px;}
#t10_23{left:164px;bottom:257px;letter-spacing:-0.14px;word-spacing:0.01px;}
#t11_23{left:1106px;bottom:251px;letter-spacing:-0.05px;}
#t12_23{left:1113px;bottom:239px;letter-spacing:0.05px;}
#t13_23{left:1190px;bottom:297px;letter-spacing:0.03px;word-spacing:0.03px;}
#t14_23{left:1269px;bottom:297px;}
#t15_23{left:1190px;bottom:286px;letter-spacing:0.04px;}
#t16_23{left:1190px;bottom:275px;letter-spacing:0.05px;word-spacing:-0.06px;}
#t17_23{left:1188px;bottom:253px;letter-spacing:0.02px;word-spacing:0.08px;}
#t18_23{left:1267px;bottom:253px;}
#t19_23{left:1188px;bottom:242px;letter-spacing:0.02px;word-spacing:0.04px;}
#t1a_23{left:1188px;bottom:220px;letter-spacing:-0.01px;word-spacing:0.1px;}
#t1b_23{left:1188px;bottom:209px;letter-spacing:0.03px;word-spacing:-0.04px;}
#t1c_23{left:1188px;bottom:198px;letter-spacing:-0.07px;}
#t1d_23{left:1014px;bottom:266px;letter-spacing:0.06px;}
#t1e_23{left:1108px;bottom:282px;letter-spacing:0.09px;}
#t1f_23{left:1179px;bottom:311px;letter-spacing:0.08px;}
#t1g_23{left:1023px;bottom:251px;letter-spacing:-0.1px;word-spacing:0.11px;}
#t1h_23{left:1022px;bottom:239px;letter-spacing:-0.07px;word-spacing:-0.07px;}
#t1i_23{left:1341px;bottom:284px;}
#t1j_23{left:1364px;bottom:284px;}
#t1k_23{left:1386px;bottom:284px;}
#t1l_23{left:1344px;bottom:249px;}
#t1m_23{left:1366px;bottom:249px;}
#t1n_23{left:1389px;bottom:249px;}
#t1o_23{left:1342px;bottom:216px;}
#t1p_23{left:1365px;bottom:216px;}
#t1q_23{left:1388px;bottom:216px;}
#t1r_23{left:105px;bottom:211px;}
#t1s_23{left:144px;bottom:210px;}
#t1t_23{left:164px;bottom:211px;letter-spacing:-0.14px;word-spacing:0.05px;}
#t1u_23{left:164px;bottom:197px;letter-spacing:-0.13px;word-spacing:0.04px;}
#t1v_23{left:144px;bottom:182px;}
#t1w_23{left:164px;bottom:184px;letter-spacing:-0.17px;word-spacing:0.09px;}
#t1x_23{left:1118px;bottom:118px;}
#t1y_23{left:1118px;bottom:104px;}
#t1z_23{left:1118px;bottom:88px;}
#t20_23{left:1111px;bottom:139px;letter-spacing:0.05px;}
#t21_23{left:1229px;bottom:102px;letter-spacing:-0.01px;}
#t22_23{left:1221px;bottom:137px;}
#t23_23{left:1229px;bottom:116px;letter-spacing:0.03px;}
#t24_23{left:1229px;bottom:86px;letter-spacing:0.03px;}
#t25_23{left:1269px;bottom:140px;letter-spacing:0.06px;word-spacing:-0.06px;}
#t26_23{left:1321px;bottom:140px;}
#t27_23{left:1325px;bottom:140px;}
#t28_23{left:1329px;bottom:140px;}
#t29_23{left:1331px;bottom:140px;}
#t2a_23{left:1354px;bottom:102px;letter-spacing:-0.01px;}
#t2b_23{left:1354px;bottom:116px;letter-spacing:-0.01px;}
#t2c_23{left:1354px;bottom:86px;letter-spacing:-0.01px;}
#t2d_23{left:1150px;bottom:138px;letter-spacing:0.08px;}
#t2e_23{left:1347px;bottom:139px;letter-spacing:0.03px;}
#t2f_23{left:1161px;bottom:156px;letter-spacing:0.07px;word-spacing:-0.08px;}
#t2g_23{left:1284px;bottom:159px;letter-spacing:0.09px;word-spacing:-0.09px;}
#t2h_23{left:105px;bottom:147px;}
#t2i_23{left:144px;bottom:145px;}
#t2j_23{left:165px;bottom:147px;letter-spacing:-0.16px;word-spacing:0.07px;}
#t2k_23{left:995px;bottom:147px;}
#t2l_23{left:1002px;bottom:147px;}
#t2m_23{left:165px;bottom:132px;letter-spacing:-0.12px;}
#t2n_23{left:105px;bottom:102px;}
#t2o_23{left:144px;bottom:101px;}
#t2p_23{left:164px;bottom:102px;letter-spacing:-0.14px;word-spacing:0.03px;}
#t2q_23{left:164px;bottom:88px;letter-spacing:-0.14px;word-spacing:-0.01px;}
#t2r_23{left:382px;bottom:88px;letter-spacing:-0.08px;word-spacing:-0.01px;}
#t2s_23{left:640px;bottom:88px;}
#t2t_23{left:144px;bottom:72px;}
#t2u_23{left:164px;bottom:75px;letter-spacing:-0.16px;word-spacing:0.09px;}
#t2v_23{left:666px;bottom:75px;}
#t2w_23{left:673px;bottom:74px;}
#t2x_23{left:675px;bottom:75px;}
#t2y_23{left:143px;bottom:371px;}
#t2z_23{left:163px;bottom:372px;letter-spacing:-0.16px;word-spacing:0.1px;}
#t30_23{left:326px;bottom:372px;letter-spacing:-0.12px;word-spacing:0.03px;}
#t31_23{left:143px;bottom:231px;}
#t32_23{left:164px;bottom:233px;letter-spacing:-0.15px;word-spacing:0.07px;}
#t33_23{left:124px;bottom:605px;letter-spacing:-0.12px;word-spacing:-0.06px;}
#t34_23{left:233px;bottom:604px;}
#t35_23{left:253px;bottom:605px;letter-spacing:-0.14px;word-spacing:0.05px;}
#t36_23{left:953px;bottom:605px;letter-spacing:-0.1px;}
#t37_23{left:253px;bottom:591px;letter-spacing:-0.15px;word-spacing:0.06px;}
#t38_23{left:253px;bottom:577px;letter-spacing:-0.14px;word-spacing:0.03px;}
#t39_23{left:570px;bottom:577px;letter-spacing:-0.14px;word-spacing:0.02px;}
#t3a_23{left:903px;bottom:577px;}
#t3b_23{left:233px;bottom:562px;}
#t3c_23{left:253px;bottom:564px;letter-spacing:-0.15px;word-spacing:0.08px;}
#t3d_23{left:253px;bottom:550px;letter-spacing:-0.06px;}

.s0_23{font-size:31px;font-family:DejaVuSans_6t;color:#000;}
.s1_23{font-size:18px;font-family:Carlito_77;color:#8B8B8B;}
.s2_23{font-size:17px;font-family:Carlito-Bold_7r;color:#000;}
.s3_23{font-size:18px;font-family:Carlito_77;color:#000;}
.s4_23{font-size:18px;font-family:OpenSymbol_7m;color:#000;}
.s5_23{font-size:14px;font-family:Carlito_77;color:#000;}
.s6_23{font-size:14px;font-family:LiberationSans_6o;color:#000;}
.s7_23{font-size:10px;font-family:Carlito_77;color:#000;}
.s8_23{font-size:9px;font-family:Carlito_77;color:#000;}
.s9_23{font-size:15px;font-family:Carlito-Bold_7r;color:#000;}
.sa_23{font-size:9px;font-family:Carlito_77;color:#808080;}
.sb_23{font-size:5px;font-family:Carlito_77;color:#000;}
.sc_23{font-size:9px;font-family:Carlito-Italic_7c;color:#000;}
.sd_23{font-size:14px;font-family:Carlito-Bold_7r;color:#000;}
.se_23{font-size:8px;font-family:Carlito-Bold_7r;color:#000;}
.sf_23{font-size:14px;font-family:Carlito-Italic_7c;color:#000;}
.sg_23{font-size:14px;font-family:Carlito_77;color:#0563C1;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts23" type="text/css" >

@font-face {
	font-family: Carlito-Bold_7r;
	src: url("fonts/Carlito-Bold_7r.woff") format("woff");
}

@font-face {
	font-family: Carlito-Italic_7c;
	src: url("fonts/Carlito-Italic_7c.woff") format("woff");
}

@font-face {
	font-family: Carlito_77;
	src: url("fonts/Carlito_77.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_6t;
	src: url("fonts/DejaVuSans_6t.woff") format("woff");
}

@font-face {
	font-family: LiberationSans_6o;
	src: url("fonts/LiberationSans_6o.woff") format("woff");
}

@font-face {
	font-family: OpenSymbol_7m;
	src: url("fonts/OpenSymbol_7m.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg23Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg23" style="-webkit-user-select: none;"><object width="1466" height="825" data="23/23.svg" type="image/svg+xml" id="pdf23" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_23" class="t s0_23">Generative AI Project Life Cycle </span><span id="t2_23" class="t s0_23">- Reinforcement learning for human feedback </span>
<span id="t3_23" class="t s1_23">Kumar </span>
<span id="t4_23" class="t s2_23">Adapt and align model </span>
<span id="t5_23" class="t s2_23">Align with human feedback </span>
<span id="t6_23" class="t s3_23">Use the reward model in the reinforcement learning process to update the LLM weights and produce a human aligned model. </span>
<span id="t7_23" class="t s4_23"> </span>
<span id="t8_23" class="t s3_23">It's important you want to start with a model that already has a good performance on your task of interests. </span>
<span id="t9_23" class="t s5_23">1 </span><span id="ta_23" class="t s6_23">• </span><span id="tb_23" class="t s5_23" data-mappings='[[4,"fi"],[17,"fi"]]'>The ﬁrst step in ﬁne-tuning an LLM with RLHF is to select a model to work with and use it to prepare a data set for human feedback. </span>
<span id="tc_23" class="t s6_23">• </span><span id="td_23" class="t s5_23" data-mappings='[[20,"fi"],[88,"fi"],[145,"ti"]]'>In general, you may ﬁnd it easier to start with an instruct model that has already been ﬁne-tuned across many tasks and has some general capabilies. </span>
<span id="te_23" class="t s7_23">Instruct </span>
<span id="tf_23" class="t s7_23">LLM </span>
<span id="tg_23" class="t s5_23">2 </span><span id="th_23" class="t s6_23">• </span><span id="ti_23" class="t s5_23" data-mappings='[[73,"ff"]]'>Then use this LLM along with a prompt data set to generate a number of diﬀerent responses for each prompt. </span>
<span id="tj_23" class="t s7_23">Prompt </span>
<span id="tk_23" class="t s7_23">dataset </span>
<span id="tl_23" class="t s8_23">Prompt samples </span><span id="tm_23" class="t s8_23" data-mappings='[[12,"ti"]]'>Model compleon </span>
<span id="tn_23" class="t s5_23">3 </span><span id="to_23" class="t s6_23">• </span><span id="tp_23" class="t s5_23">The next step is to collect feedback from human labellers </span><span id="tq_23" class="t s5_23" data-mappings='[[13,"ti"],[70,"ti"]]'>on the compleons generated by the LLM. This is the human feedback poron of </span>
<span id="tr_23" class="t s5_23">reinforcement. </span>
<span id="ts_23" class="t s6_23">• </span><span id="tt_23" class="t s5_23" data-mappings='[[78,"ti"]]'>First, you must decide what criterion you want the humans to assess the compleons on. This could be any of the issues like helpfulness or toxicity. </span>
<span id="tu_23" class="t s9_23">Steps to obtain Feedback from humans for RLHF </span>
<span id="tv_23" class="t s5_23">4 </span><span id="tw_23" class="t s6_23">• </span><span id="tx_23" class="t s5_23" data-mappings='[[66,"ti"]]'>The task for labellers in this example is to rank the three compleons in order of helpfulness from the most helpful to least helpful. </span>
<span id="ty_23" class="t s6_23">• </span><span id="tz_23" class="t s5_23" data-mappings='[[70,"ti"]]'>Once human labellers have completed their assessments of prompt compleon sets, you have the data to train the reward model. Which will use </span>
<span id="t10_23" class="t s5_23" data-mappings='[[42,"ti"],[81,"fi"]]'>instead of humans to classify model compleons during the reinforcement learning ﬁnetuning process. </span>
<span id="t11_23" class="t s7_23">Instruct </span>
<span id="t12_23" class="t s7_23">LLM </span>
<span id="t13_23" class="t sa_23">My laptop is too slow</span><span id="t14_23" class="t s8_23">. </span>
<span id="t15_23" class="t s8_23">There is nothing you can do about </span>
<span id="t16_23" class="t s8_23">slow laptop </span>
<span id="t17_23" class="t sa_23">My laptop is too slow</span><span id="t18_23" class="t s8_23">. </span>
<span id="t19_23" class="t s8_23" data-mappings='[[30,"ti"]]'>Try closing few unused applicaons </span>
<span id="t1a_23" class="t sa_23">My laptop is too slow. </span>
<span id="t1b_23" class="t s8_23" data-mappings='[[12,"fi"],[17,"ti"]]'>For this conﬁguraon it is not too </span>
<span id="t1c_23" class="t s8_23">slow. </span>
<span id="t1d_23" class="t s8_23">Prompt </span>
<span id="t1e_23" class="t s8_23">Model </span>
<span id="t1f_23" class="t s8_23" data-mappings='[[6,"ti"]]'>Compleon </span>
<span id="t1g_23" class="t s7_23">My laptop </span>
<span id="t1h_23" class="t s7_23">is too slow </span>
<span id="t1i_23" class="t s5_23">2 </span><span id="t1j_23" class="t s5_23">2 </span><span id="t1k_23" class="t s5_23">2 </span>
<span id="t1l_23" class="t s5_23">3 </span><span id="t1m_23" class="t s5_23">1 </span><span id="t1n_23" class="t s5_23">1 </span>
<span id="t1o_23" class="t s5_23">1 </span><span id="t1p_23" class="t s5_23">3 </span><span id="t1q_23" class="t s5_23">3 </span>
<span id="t1r_23" class="t s5_23">5 </span><span id="t1s_23" class="t s6_23">• </span><span id="t1t_23" class="t s5_23" data-mappings='[[107,"ti"],[145,"ti"]]'>Before training the reward model, you need to convert the ranking data into a pairwise comparison of compleons i.e. all possible pairs of compleons </span>
<span id="t1u_23" class="t s5_23" data-mappings='[[16,"fi"]]'>should be classiﬁed as 0 or 1 score. </span>
<span id="t1v_23" class="t s6_23">• </span><span id="t1w_23" class="t s5_23">For each pair, you will assign a reward of 1 for the preferred response and a reward of 0 for the less preferred response. </span>
<span id="t1x_23" class="t s8_23">2 </span>
<span id="t1y_23" class="t s8_23">1 </span>
<span id="t1z_23" class="t s8_23">3 </span>
<span id="t20_23" class="t s8_23">Rank </span>
<span id="t21_23" class="t s8_23">[1,0] </span>
<span id="t22_23" class="t s8_23">Reward </span>
<span id="t23_23" class="t s8_23">[0,1] </span>
<span id="t24_23" class="t s8_23">[1,0] </span>
<span id="t25_23" class="t s8_23" data-mappings='[[6,"ti"]]'>Compleon (y </span>
<span id="t26_23" class="t sb_23">j, </span>
<span id="t27_23" class="t s8_23">y </span>
<span id="t28_23" class="t sb_23">j </span>
<span id="t29_23" class="t s8_23">) </span>
<span id="t2a_23" class="t s8_23">[1,0] </span>
<span id="t2b_23" class="t s8_23">[1,0] </span>
<span id="t2c_23" class="t s8_23">[1,0] </span>
<span id="t2d_23" class="t s8_23" data-mappings='[[6,"ti"]]'>Compleon </span><span id="t2e_23" class="t s8_23">Reward </span>
<span id="t2f_23" class="t sc_23">pairwise reward </span>
<span id="t2g_23" class="t sc_23">reorder reward </span>
<span id="t2h_23" class="t s5_23">6 </span>
<span id="t2i_23" class="t s6_23">• </span>
<span id="t2j_23" class="t s5_23" data-mappings='[[56,"ti"],[66,"fi"],[117,"ti"]]'>Then you'll reorder the prompts so that the preferred opon comes ﬁrst. The reward model expects the preferred compleon, which is referred to as </span><span id="t2k_23" class="t sd_23">Y </span>
<span id="t2l_23" class="t se_23">j </span>
<span id="t2m_23" class="t sd_23" data-mappings='[[0,"fi"]]'>ﬁrst. </span>
<span id="t2n_23" class="t s5_23">7 </span><span id="t2o_23" class="t s6_23">• </span><span id="t2p_23" class="t s5_23" data-mappings='[[63,"ti"],[121,"fi"]]'>Once the model has been trained on the human rank prompt-compleon pairs, you can use the reward model as a binary classiﬁer to provide reward </span>
<span id="t2q_23" class="t s5_23" data-mappings='[[28,"ti"]]'>value for each prompt-compleon pair </span><span id="t2r_23" class="t sf_23">[e.g. +ve Logits(not hate) and -ve Logits(hate)]</span><span id="t2s_23" class="t s5_23">. </span>
<span id="t2t_23" class="t s6_23">• </span>
<span id="t2u_23" class="t s5_23" data-mappings='[[82,"ti"]]'>For a given prompt X, the reward model learns to favour the human-preferred compleon </span><span id="t2v_23" class="t sd_23">Y </span>
<span id="t2w_23" class="t se_23">j </span>
<span id="t2x_23" class="t s5_23">, </span>
<span id="t2y_23" class="t s6_23">• </span><span id="t2z_23" class="t s5_23">The prompt dataset consists </span><span id="t30_23" class="t s5_23" data-mappings='[[6,"ti"],[86,"ti"]]'>of mulple prompts, each of which gets processed by the LLM to produce a set of compleons. </span>
<span id="t31_23" class="t s6_23">• </span><span id="t32_23" class="t s5_23" data-mappings='[[13,"fi"],[114,"ti"]]'>In this e.g. ﬁrst labeller responses disagree with the others and may indicate that they misunderstood the instrucons. </span>
<span id="t33_23" class="t sd_23">Why RLHF </span><span id="t34_23" class="t s6_23">• </span><span id="t35_23" class="t s5_23" data-mappings='[[74,"fi"]]'>In 2020, researchers at OpenAI published a paper that explored the use of ﬁne-tuning with human feedback to train a model. </span><span id="t36_23" class="t s5_23">The </span>
<span id="t37_23" class="t s5_23" data-mappings='[[2,"ti"],[26,"fi"],[65,"tt"],[116,"fi"]]'>arcle demonstrates model ﬁne-tuned on human feedback produced beer responses than a pretrained model, an instruct ﬁne-tuned </span>
<span id="t38_23" class="t s5_23">model, and even the reference human baseline. (source: </span><span id="t39_23" class="t sg_23">OpenAI paper Learning to summarize from human feedback </span><span id="t3a_23" class="t s5_23">) </span>
<span id="t3b_23" class="t s6_23">• </span><span id="t3c_23" class="t s5_23" data-mappings='[[23,"fi"]]'>A popular technique to ﬁnetune large language models with human feedback is called reinforcement learning from human feedback </span>
<span id="t3d_23" class="t s5_23">(RLHF). </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
