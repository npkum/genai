<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p15" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_15{left:110px;bottom:756px;letter-spacing:-0.06px;word-spacing:0.13px;}
#t2_15{left:112px;bottom:724px;letter-spacing:0.12px;word-spacing:0.09px;}
#t3_15{left:112px;bottom:702px;letter-spacing:0.13px;word-spacing:0.04px;}
#t4_15{left:368px;bottom:702px;letter-spacing:0.17px;word-spacing:-0.05px;}
#t5_15{left:112px;bottom:665px;letter-spacing:0.12px;word-spacing:-0.09px;}
#t6_15{left:175px;bottom:664px;letter-spacing:0.12px;word-spacing:0.09px;}
#t7_15{left:112px;bottom:642px;letter-spacing:0.11px;word-spacing:0.07px;}
#t8_15{left:112px;bottom:620px;letter-spacing:0.12px;word-spacing:0.08px;}
#t9_15{left:112px;bottom:598px;letter-spacing:0.11px;}
#ta_15{left:112px;bottom:563px;letter-spacing:-0.1px;word-spacing:0.07px;}
#tb_15{left:110px;bottom:536px;letter-spacing:0.08px;}
#tc_15{left:257px;bottom:536px;letter-spacing:0.07px;word-spacing:0.02px;}
#td_15{left:110px;bottom:510px;letter-spacing:0.04px;word-spacing:0.04px;}
#te_15{left:110px;bottom:497px;letter-spacing:0.1px;}
#tf_15{left:257px;bottom:510px;letter-spacing:0.05px;word-spacing:0.02px;}
#tg_15{left:257px;bottom:497px;letter-spacing:0.05px;word-spacing:0.05px;}
#th_15{left:110px;bottom:469px;letter-spacing:0.07px;}
#ti_15{left:257px;bottom:469px;letter-spacing:0.06px;word-spacing:0.03px;}
#tj_15{left:112px;bottom:428px;letter-spacing:0.13px;word-spacing:-0.09px;}
#tk_15{left:178px;bottom:427px;letter-spacing:0.15px;word-spacing:0.05px;}
#tl_15{left:112px;bottom:405px;letter-spacing:0.12px;word-spacing:0.08px;}
#tm_15{left:112px;bottom:383px;letter-spacing:0.08px;}
#tn_15{left:112px;bottom:361px;letter-spacing:0.12px;word-spacing:0.05px;}
#to_15{left:112px;bottom:339px;letter-spacing:0.11px;word-spacing:0.09px;}
#tp_15{left:112px;bottom:305px;letter-spacing:-0.1px;word-spacing:0.05px;}
#tq_15{left:112px;bottom:284px;letter-spacing:-0.09px;word-spacing:0.04px;}
#tr_15{left:112px;bottom:264px;letter-spacing:-0.08px;word-spacing:0.15px;}
#ts_15{left:112px;bottom:244px;letter-spacing:-0.16px;word-spacing:0.09px;}
#tt_15{left:233px;bottom:244px;letter-spacing:-0.09px;word-spacing:0.05px;}
#tu_15{left:112px;bottom:224px;letter-spacing:-0.11px;word-spacing:0.06px;}
#tv_15{left:112px;bottom:204px;letter-spacing:-0.11px;}
#tw_15{left:185px;bottom:204px;letter-spacing:-0.08px;word-spacing:0.03px;}
#tx_15{left:761px;bottom:204px;letter-spacing:-0.15px;}
#ty_15{left:799px;bottom:204px;letter-spacing:-0.09px;word-spacing:0.03px;}
#tz_15{left:532px;bottom:185px;letter-spacing:-0.2px;}
#t10_15{left:533px;bottom:172px;letter-spacing:-0.18px;}
#t11_15{left:534px;bottom:159px;letter-spacing:-0.14px;}
#t12_15{left:533px;bottom:147px;letter-spacing:-0.2px;}
#t13_15{left:532px;bottom:116px;letter-spacing:-0.2px;}
#t14_15{left:534px;bottom:103px;letter-spacing:-0.24px;}
#t15_15{left:517px;bottom:66px;letter-spacing:-0.19px;word-spacing:-0.01px;}
#t16_15{left:534px;bottom:53px;letter-spacing:-0.14px;}
#t17_15{left:533px;bottom:40px;letter-spacing:-0.2px;}
#t18_15{left:794px;bottom:179px;letter-spacing:-0.5px;}
#t19_15{left:793px;bottom:166px;letter-spacing:-0.16px;}
#t1a_15{left:775px;bottom:154px;letter-spacing:-0.15px;word-spacing:-0.1px;}
#t1b_15{left:776px;bottom:110px;letter-spacing:-0.37px;word-spacing:0.18px;}
#t1c_15{left:779px;bottom:60px;letter-spacing:-0.36px;word-spacing:0.17px;}
#t1d_15{left:775px;bottom:47px;letter-spacing:-0.15px;word-spacing:-0.1px;}
#t1e_15{left:638px;bottom:118px;letter-spacing:-0.03px;word-spacing:0.1px;}
#t1f_15{left:652px;bottom:104px;letter-spacing:0.07px;}
#t1g_15{left:1267px;bottom:784px;letter-spacing:-0.09px;}
#t1h_15{left:1199px;bottom:764px;letter-spacing:-0.11px;word-spacing:0.07px;}

.s0_15{font-size:31px;font-family:DejaVuSans_7l;color:#000;}
.s1_15{font-size:18px;font-family:Carlito-Bold_8j;color:#000;}
.s2_15{font-size:18px;font-family:Carlito_7-;color:#333;}
.s3_15{font-size:15px;font-family:Carlito-Bold_8j;color:#000;}
.s4_15{font-size:18px;font-family:Carlito_7-;color:#081819;}
.s5_15{font-size:17px;font-family:Carlito-Bold_8j;color:#000;}
.s6_15{font-size:12px;font-family:Carlito-Bold_8j;color:#000;}
.s7_15{font-size:12px;font-family:Carlito_7-;color:#000;}
.s8_15{font-size:17px;font-family:Carlito_7-;color:#081819;}
.s9_15{font-size:17px;font-family:Carlito-Bold_8j;color:#081819;}
.sa_15{font-size:17px;font-family:Carlito_7-;color:#0563C1;}
.sb_15{font-size:11px;font-family:DejaVuSans_7l;color:#000;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts15" type="text/css" >

@font-face {
	font-family: Carlito-Bold_8j;
	src: url("fonts/Carlito-Bold_8j.woff") format("woff");
}

@font-face {
	font-family: Carlito_7-;
	src: url("fonts/Carlito_7-.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_7l;
	src: url("fonts/DejaVuSans_7l.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg15Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg15" style="-webkit-user-select: none;"><object width="1466" height="825" data="15/15.svg" type="image/svg+xml" id="pdf15" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_15" class="t s0_15">Generative AI Project Life Cycle - Choose existing model or pre-train </span>
<span id="t2_15" class="t s1_15" data-mappings='[[28,"fi"]]'>How to create a Domain-speciﬁc LLM </span>
<span id="t3_15" class="t s2_15" data-mappings='[[16,"ti"]]'>There are two opons to develop </span><span id="t4_15" class="t s2_15" data-mappings='[[12,"fi"]]'>domain-speciﬁc models. </span>
<span id="t5_15" class="t s3_15" data-mappings='[[2,"ti"]]'>Opon1 - </span><span id="t6_15" class="t s1_15" data-mappings='[[11,"ti"],[27,"fi"]]'>Build an enre domain-speciﬁc model from scratch </span>
<span id="t7_15" class="t s4_15" data-mappings='[[22,"ti"],[36,"ti"],[80,"fi"],[110,"tti"]]'>You can train a foundaonal model enrely from a blank slate with industry-speciﬁc knowledge. This involves geng the model to learn self-supervised with unlabelled data. </span>
<span id="t8_15" class="t s4_15" data-mappings='[[52,"ti"],[93,"tt"],[123,"ti"],[145,"fi"]]'>During training, the model applies next-token predicon and mask-level modelling. The model aempts to predict words sequenally by masking speciﬁc tokens in a </span>
<span id="t9_15" class="t s4_15">sentence. </span>
<span id="ta_15" class="t s5_15" data-mappings='[[28,"ti"],[44,"fi"]]'>Challenges of building an enre domain-speciﬁc model from scratch </span>
<span id="tb_15" class="t s6_15">Data challenges </span><span id="tc_15" class="t s7_15" data-mappings='[[26,"ti"],[42,"fi"],[117,"fi"],[121,"ti"]]'>The procurement of substanal, niche-speciﬁc data could be demanding, especially when dealing with specialized or conﬁdenal data. </span>
<span id="td_15" class="t s6_15">Technical and Resource </span>
<span id="te_15" class="t s6_15">challenges </span>
<span id="tf_15" class="t s7_15" data-mappings='[[5,"ti"],[124,"ti"],[203,"fi"]]'>Selecon of suitable architecture and parameters necessitates specialized knowledge and comes with considerable cost. Evaluaon becomes complex owing to the lack of established benchmarks for niche-speciﬁc tasks, and accuracy, </span>
<span id="tg_15" class="t s7_15" data-mappings='[[29,"ti"],[64,"ti"]]'>safety, and compliance validaon of model responses present addional challenges. </span>
<span id="th_15" class="t s6_15">Ethical challenges </span><span id="ti_15" class="t s7_15" data-mappings='[[21,"ti"],[69,"ti"],[133,"fi"]]'>Robust content moderaon mechanisms must be in place to prevent potenally inappropriate or harmful content generated by domain-speciﬁc LLMs. </span>
<span id="tj_15" class="t s3_15" data-mappings='[[2,"ti"]]'>Opon2 – </span><span id="tk_15" class="t s1_15" data-mappings='[[33,"fi"]]'>Fine-tune an LLM for domain-speciﬁc needs </span>
<span id="tl_15" class="t s4_15" data-mappings='[[47,"fi"],[116,"ti"],[127,"ti"],[165,"ti"]]'>Not all use cases require to train domain-speciﬁc models from scratch especial where output of LLM is used as indicave informaon only and where there is costs and me </span>
<span id="tm_15" class="t s4_15">constrain. </span>
<span id="tn_15" class="t s4_15" data-mappings='[[20,"fi"],[39,"ti"],[56,"ffi"],[81,"fi"]]'>In these use cases, ﬁne-tuning a foundaonal model is suﬃcient to perform a speciﬁc task with reasonable accuracy. This approach has lesser challenges i.e. requires lesser </span>
<span id="to_15" class="t s4_15" data-mappings='[[17,"ti"],[26,"ti"]]'>datasets, computaon, and me. </span>
<span id="tp_15" class="t s8_15" data-mappings='[[5,"fi"],[98,"ti"],[111,"ti"],[132,"fi"]]'>When ﬁne-tuning an LLM, select a pre-trained model like GPT and LLaMa, which already possess exceponal linguisc capability. And reﬁne the model’s weight by training it with a small set </span>
<span id="tq_15" class="t s8_15" data-mappings='[[62,"fi"],[170,"ti"]]'>of annotated data with a slow learning rate. The principle of ﬁne-tuning enables the language model to adopt the knowledge that new data presents while retaining the exisng ones it </span>
<span id="tr_15" class="t s8_15" data-mappings='[[3,"ti"]]'>inially learned. </span>
<span id="ts_15" class="t s9_15">Transfer learning </span><span id="tt_15" class="t s8_15" data-mappings='[[135,"ffi"],[154,"fi"]]'>is a unique technique that allows a pre-trained model to apply its knowledge to a new task. It is instrumental when you can’t curate suﬃcient datasets to ﬁne-tune a model. </span>
<span id="tu_15" class="t s8_15" data-mappings='[[36,"ti"]]'>In transfer learning the models exisng weights/layers are feezed and appended with new trainable ones to the top. </span>
<span id="tv_15" class="t s8_15">MedPaLM </span><span id="tw_15" class="t s8_15" data-mappings='[[31,"fi"]]'>is an example of a domain-speciﬁc model trained with this approach. It is built upon </span><span id="tx_15" class="t sa_15">PaLM </span><span id="ty_15" class="t s8_15">, a 540 billion parameters language model. </span>
<span id="tz_15" class="t sb_15">Source </span>
<span id="t10_15" class="t sb_15">Labels </span>
<span id="t11_15" class="t sb_15">(Size : </span>
<span id="t12_15" class="t sb_15">Large) </span>
<span id="t13_15" class="t sb_15">Source </span>
<span id="t14_15" class="t sb_15">Model </span>
<span id="t15_15" class="t sb_15">Source Data </span>
<span id="t16_15" class="t sb_15">(Size : </span>
<span id="t17_15" class="t sb_15">Large) </span>
<span id="t18_15" class="t sb_15">Target </span>
<span id="t19_15" class="t sb_15">Labels </span>
<span id="t1a_15" class="t sb_15">(Size : Small) </span>
<span id="t1b_15" class="t sb_15">Target Model </span>
<span id="t1c_15" class="t sb_15">Target Data </span>
<span id="t1d_15" class="t sb_15">(Size : Small) </span>
<span id="t1e_15" class="t s7_15">Transfer Learned </span>
<span id="t1f_15" class="t s7_15">Knowledge </span>
<span id="t1g_15" class="t s5_15">Select </span>
<span id="t1h_15" class="t s5_15">Pre-train your own model </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
