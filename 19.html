<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p19" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_19{left:112px;bottom:740px;letter-spacing:0.01px;word-spacing:0.08px;}
#t2_19{left:736px;bottom:740px;}
#t3_19{left:762px;bottom:741px;letter-spacing:-0.15px;}
#t4_19{left:123px;bottom:705px;letter-spacing:0.12px;word-spacing:0.06px;}
#t5_19{left:123px;bottom:683px;letter-spacing:0.13px;word-spacing:0.08px;}
#t6_19{left:123px;bottom:643px;letter-spacing:0.04px;word-spacing:0.16px;}
#t7_19{left:123px;bottom:622px;}
#t8_19{left:143px;bottom:621px;letter-spacing:0.12px;word-spacing:0.04px;}
#t9_19{left:123px;bottom:600px;}
#ta_19{left:143px;bottom:599px;letter-spacing:0.09px;word-spacing:0.11px;}
#tb_19{left:143px;bottom:577px;letter-spacing:0.1px;word-spacing:0.08px;}
#tc_19{left:477px;bottom:577px;letter-spacing:0.17px;}
#td_19{left:561px;bottom:577px;letter-spacing:0.12px;word-spacing:0.05px;}
#te_19{left:123px;bottom:509px;letter-spacing:-0.12px;}
#tf_19{left:253px;bottom:508px;}
#tg_19{left:274px;bottom:509px;letter-spacing:-0.14px;word-spacing:0.03px;}
#th_19{left:124px;bottom:546px;letter-spacing:0.1px;word-spacing:0.08px;}
#ti_19{left:1055px;bottom:507px;letter-spacing:-0.19px;}
#tj_19{left:1069px;bottom:494px;letter-spacing:-0.23px;}
#tk_19{left:1321px;bottom:507px;letter-spacing:-0.16px;}
#tl_19{left:1309px;bottom:494px;letter-spacing:-0.22px;word-spacing:-0.06px;}
#tm_19{left:1062px;bottom:541px;letter-spacing:0.1px;}
#tn_19{left:1309px;bottom:543px;letter-spacing:0.1px;}
#to_19{left:1164px;bottom:517px;letter-spacing:-0.24px;word-spacing:0.14px;}
#tp_19{left:1164px;bottom:504px;letter-spacing:-0.24px;word-spacing:0.14px;}
#tq_19{left:1164px;bottom:491px;letter-spacing:-0.24px;word-spacing:0.14px;}
#tr_19{left:1168px;bottom:539px;letter-spacing:0.09px;}
#ts_19{left:1089px;bottom:463px;letter-spacing:-0.16px;word-spacing:0.04px;}
#tt_19{left:253px;bottom:480px;}
#tu_19{left:274px;bottom:481px;letter-spacing:-0.15px;word-spacing:0.09px;}
#tv_19{left:274px;bottom:467px;letter-spacing:-0.14px;word-spacing:0.06px;}
#tw_19{left:1055px;bottom:430px;letter-spacing:-0.2px;}
#tx_19{left:1068px;bottom:417px;letter-spacing:-0.31px;}
#ty_19{left:1321px;bottom:430px;letter-spacing:-0.15px;}
#tz_19{left:1309px;bottom:417px;letter-spacing:-0.26px;word-spacing:0.14px;}
#t10_19{left:1145px;bottom:439px;letter-spacing:-0.19px;word-spacing:-0.08px;}
#t11_19{left:1145px;bottom:426px;letter-spacing:-0.19px;word-spacing:-0.08px;}
#t12_19{left:1145px;bottom:413px;letter-spacing:-0.19px;word-spacing:-0.08px;}
#t13_19{left:1255px;bottom:387px;letter-spacing:-0.22px;word-spacing:-0.04px;}
#t14_19{left:131px;bottom:344px;letter-spacing:-0.13px;}
#t15_19{left:155px;bottom:330px;letter-spacing:-0.14px;}
#t16_19{left:253px;bottom:344px;letter-spacing:-0.15px;word-spacing:0.09px;}
#t17_19{left:253px;bottom:330px;letter-spacing:-0.15px;word-spacing:0.08px;}
#t18_19{left:253px;bottom:316px;letter-spacing:-0.15px;word-spacing:0.04px;}
#t19_19{left:253px;bottom:302px;letter-spacing:-0.14px;word-spacing:0.05px;}
#t1a_19{left:746px;bottom:302px;letter-spacing:-0.18px;word-spacing:0.03px;}
#t1b_19{left:871px;bottom:302px;}
#t1c_19{left:1060px;bottom:337px;letter-spacing:-0.21px;}
#t1d_19{left:1073px;bottom:324px;letter-spacing:-0.23px;}
#t1e_19{left:1326px;bottom:336px;letter-spacing:-0.19px;}
#t1f_19{left:1314px;bottom:324px;letter-spacing:-0.24px;word-spacing:0.12px;}
#t1g_19{left:1199px;bottom:260px;letter-spacing:-0.2px;word-spacing:0.04px;}
#t1h_19{left:1225px;bottom:247px;letter-spacing:-0.17px;word-spacing:0.02px;}
#t1i_19{left:1146px;bottom:357px;letter-spacing:-0.11px;word-spacing:0.04px;}
#t1j_19{left:1185px;bottom:357px;letter-spacing:-0.15px;word-spacing:0.07px;}
#t1k_19{left:1219px;bottom:357px;letter-spacing:-0.23px;word-spacing:-0.01px;}
#t1l_19{left:1146px;bottom:344px;letter-spacing:-0.22px;}
#t1m_19{left:1177px;bottom:344px;letter-spacing:-0.19px;}
#t1n_19{left:1226px;bottom:344px;letter-spacing:-0.18px;}
#t1o_19{left:1146px;bottom:316px;letter-spacing:-0.11px;word-spacing:0.04px;}
#t1p_19{left:1185px;bottom:316px;letter-spacing:-0.15px;}
#t1q_19{left:1146px;bottom:303px;letter-spacing:-0.14px;}
#t1r_19{left:1174px;bottom:303px;letter-spacing:-0.2px;}
#t1s_19{left:1223px;bottom:303px;letter-spacing:-0.27px;}
#t1t_19{left:140px;bottom:212px;letter-spacing:-0.18px;word-spacing:0.1px;}
#t1u_19{left:143px;bottom:199px;letter-spacing:-0.17px;}
#t1v_19{left:150px;bottom:185px;letter-spacing:-0.19px;}
#t1w_19{left:255px;bottom:212px;letter-spacing:-0.12px;}
#t1x_19{left:255px;bottom:197px;}
#t1y_19{left:276px;bottom:199px;letter-spacing:-0.13px;word-spacing:0.02px;}
#t1z_19{left:1198px;bottom:199px;letter-spacing:-0.13px;word-spacing:0.05px;}
#t20_19{left:276px;bottom:185px;letter-spacing:-0.16px;word-spacing:0.15px;}
#t21_19{left:255px;bottom:170px;}
#t22_19{left:276px;bottom:171px;letter-spacing:-0.15px;word-spacing:0.08px;}
#t23_19{left:932px;bottom:171px;letter-spacing:-0.13px;}
#t24_19{left:993px;bottom:171px;letter-spacing:-0.15px;word-spacing:0.06px;}
#t25_19{left:276px;bottom:155px;letter-spacing:-0.15px;word-spacing:0.07px;}
#t26_19{left:594px;bottom:155px;}
#t27_19{left:605px;bottom:155px;letter-spacing:-0.17px;word-spacing:0.1px;}
#t28_19{left:255px;bottom:124px;letter-spacing:-0.12px;}
#t29_19{left:255px;bottom:109px;}
#t2a_19{left:276px;bottom:110px;letter-spacing:-0.22px;}
#t2b_19{left:324px;bottom:110px;}
#t2c_19{left:332px;bottom:110px;letter-spacing:-0.14px;}
#t2d_19{left:386px;bottom:110px;}
#t2e_19{left:393px;bottom:110px;letter-spacing:-0.11px;}
#t2f_19{left:435px;bottom:110px;}
#t2g_19{left:441px;bottom:110px;letter-spacing:-0.16px;}
#t2h_19{left:458px;bottom:110px;letter-spacing:-0.16px;}
#t2i_19{left:467px;bottom:110px;letter-spacing:-0.14px;word-spacing:0.13px;}
#t2j_19{left:505px;bottom:110px;letter-spacing:-0.07px;}
#t2k_19{left:533px;bottom:110px;letter-spacing:-0.14px;word-spacing:0.05px;}
#t2l_19{left:255px;bottom:95px;}
#t2m_19{left:276px;bottom:96px;letter-spacing:-0.16px;word-spacing:0.08px;}
#t2n_19{left:1202px;bottom:782px;letter-spacing:-0.09px;word-spacing:0.05px;}
#t2o_19{left:1241px;bottom:762px;letter-spacing:-0.08px;}
#t2p_19{left:99px;bottom:24px;letter-spacing:0.1px;}

.s0_19{font-size:38px;font-family:DejaVuSans_7l;color:#000;}
.s1_19{font-size:35px;font-family:DejaVuSans_7l;color:#000;}
.s2_19{font-size:18px;font-family:Carlito_7-;color:#000;}
.s3_19{font-size:18px;font-family:LiberationSans_7g;color:#000;}
.s4_19{font-size:18px;font-family:Carlito-Bold_8j;color:#000;}
.s5_19{font-size:14px;font-family:Carlito-Bold_8j;color:#000;}
.s6_19{font-size:14px;font-family:LiberationSans_7g;color:#000;}
.s7_19{font-size:14px;font-family:Carlito_7-;color:#000;}
.s8_19{font-size:15px;font-family:Carlito-Bold_8j;color:#000;}
.s9_19{font-size:11px;font-family:Carlito_7-;color:#000;}
.sa_19{font-size:12px;font-family:Carlito_7-;color:#000;}
.sb_19{font-size:11px;font-family:Carlito_7-;color:#404040;}
.sc_19{font-size:11px;font-family:Carlito_7-;color:#C55A11;}
.sd_19{font-size:11px;font-family:Carlito_7-;color:#203864;}
.se_19{font-size:14px;font-family:DejaVuSans_7l;color:#000;}
.sf_19{font-size:17px;font-family:Carlito-Bold_8j;color:#000;}
.sg_19{font-size:15px;font-family:Carlito_7-;color:#0563C1;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts19" type="text/css" >

@font-face {
	font-family: Carlito-Bold_8j;
	src: url("fonts/Carlito-Bold_8j.woff") format("woff");
}

@font-face {
	font-family: Carlito_7-;
	src: url("fonts/Carlito_7-.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_7l;
	src: url("fonts/DejaVuSans_7l.woff") format("woff");
}

@font-face {
	font-family: LiberationSans_7g;
	src: url("fonts/LiberationSans_7g.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg19Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg19" style="-webkit-user-select: none;"><object width="1466" height="825" data="19/19.svg" type="image/svg+xml" id="pdf19" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_19" class="t s0_19">Generative AI Project Life Cycle </span><span id="t2_19" class="t s0_19">- </span><span id="t3_19" class="t s1_19">Fine-tunning </span>
<span id="t4_19" class="t s2_19">By performing prompt engineering i.e. including one or more examples of what you want the model to do, known as one shot or few shot inference, can be enough to </span>
<span id="t5_19" class="t s2_19" data-mappings='[[19,"ti"],[58,"ti"]]'>help the model idenfy the task and generate a good compleon. </span>
<span id="t6_19" class="t s2_19">However, this strategy has a couple of drawbacks. </span>
<span id="t7_19" class="t s3_19">• </span><span id="t8_19" class="t s2_19" data-mappings='[[54,"fi"]]'>for smaller models, it doesn't always work, even when ﬁve or six examples are included </span>
<span id="t9_19" class="t s3_19">• </span><span id="ta_19" class="t s2_19" data-mappings='[[154,"ti"]]'>any examples you include in your prompt take up valuable space in the context window, reducing the amount of room you have to include other useful informaon. To </span>
<span id="tb_19" class="t s2_19" data-mappings='[[26,"ti"]]'>overcome this another soluon exists that is </span><span id="tc_19" class="t s4_19" data-mappings='[[0,"fi"]]'>ﬁne-tuning</span><span id="td_19" class="t s2_19">. This is process further trains a base model. </span>
<span id="te_19" class="t s5_19">Fine-tunning </span><span id="tf_19" class="t s6_19">• </span><span id="tg_19" class="t s7_19">Fine-tuning is a supervised learning process where you use a data set of labelled examples to update the weights of the LLM. </span>
<span id="th_19" class="t s8_19" data-mappings='[[4,"fi"]]'>LLM ﬁne-tunning overview </span>
<span id="ti_19" class="t s9_19">Pretrained </span>
<span id="tj_19" class="t s9_19">LLM </span>
<span id="tk_19" class="t s9_19">Fine- </span>
<span id="tl_19" class="t s9_19">tuned LLM </span>
<span id="tm_19" class="t sa_19">Model </span><span id="tn_19" class="t sa_19">Model </span>
<span id="to_19" class="t sb_19">Text[…], Label[…] </span>
<span id="tp_19" class="t sb_19">Text[…], Label[…] </span>
<span id="tq_19" class="t sb_19">Text[…], Label[…] </span>
<span id="tr_19" class="t s9_19" data-mappings='[[10,"fi"]]'>Task-speciﬁc </span>
<span id="ts_19" class="t s9_19" data-mappings='[[32,"fi"]]'>GB-TB of labelled data for speciﬁc set of task </span>
<span id="tt_19" class="t s6_19">• </span><span id="tu_19" class="t s7_19" data-mappings='[[39,"ti"],[54,"fi"]]'>The labelled examples are prompt compleon pairs, the ﬁne-tuning process extends the training of the model to improve its </span>
<span id="tv_19" class="t s7_19" data-mappings='[[31,"ti"],[47,"fi"]]'>ability to generate good compleons for a speciﬁc task. </span>
<span id="tw_19" class="t s9_19">Pretrained </span>
<span id="tx_19" class="t s9_19">LLM </span>
<span id="ty_19" class="t s9_19">Fine- </span>
<span id="tz_19" class="t s9_19">tuned LLM </span>
<span id="t10_19" class="t sb_19" data-mappings='[[17,"ti"]]'>Prompt[…], Compleon[…] </span>
<span id="t11_19" class="t sb_19" data-mappings='[[17,"ti"]]'>Prompt[…], Compleon[…] </span>
<span id="t12_19" class="t sb_19" data-mappings='[[17,"ti"]]'>Prompt[…], Compleon[…] </span>
<span id="t13_19" class="t s9_19" data-mappings='[[13,"ti"]]'>prompt-compleon pairs </span>
<span id="t14_19" class="t s5_19" data-mappings='[[7,"ti"],[11,"fi"]]'>Instrucon-ﬁne- </span>
<span id="t15_19" class="t s5_19">tunning </span>
<span id="t16_19" class="t s7_19" data-mappings='[[4,"fi"],[28,"ti"],[62,"fi"]]'>The ﬁne-tunning with instrucon prompts is most common way to ﬁne-tune LLMs. It trains the model using examples that </span>
<span id="t17_19" class="t s7_19" data-mappings='[[44,"fi"],[54,"ti"]]'>demonstrate how it should respond to a speciﬁc instrucon. </span>
<span id="t18_19" class="t s7_19" data-mappings='[[7,"ti"],[11,"fi"]]'>Instrucon-ﬁne-tuning, where all of model’s weights are updated and results in new version of model with updated weights. </span>
<span id="t19_19" class="t s7_19" data-mappings='[[9,"ti"],[25,"fi"]]'>The potenal downside to ﬁne-tuning on a single task may lead to a phenomenon called </span><span id="t1a_19" class="t s7_19" data-mappings='[[18,"tti"]]'>catastrophic forgeng </span><span id="t1b_19" class="t s7_19">. </span>
<span id="t1c_19" class="t s9_19">Pretrained </span>
<span id="t1d_19" class="t s9_19">LLM </span>
<span id="t1e_19" class="t s9_19">Fine- </span>
<span id="t1f_19" class="t s9_19">tuned LLM </span>
<span id="t1g_19" class="t s9_19" data-mappings='[[18,"ti"]]'>each prompt/compleon pair includes </span>
<span id="t1h_19" class="t s9_19" data-mappings='[[5,"fi"],[15,"ti"]]'>speciﬁc instrucon to LLM </span>
<span id="t1i_19" class="t sc_19">classify : </span><span id="t1j_19" class="t s9_19">"It was </span><span id="t1k_19" class="t s9_19">great day </span>
<span id="t1l_19" class="t s9_19">today" </span><span id="t1m_19" class="t sc_19" data-mappings='[[3,"ti"]]'>senment: </span><span id="t1n_19" class="t sd_19" data-mappings='[[4,"ti"]]'>posive </span>
<span id="t1o_19" class="t sc_19">classify : </span><span id="t1p_19" class="t s9_19">“The weather is not </span>
<span id="t1q_19" class="t s9_19">good" </span><span id="t1r_19" class="t sc_19" data-mappings='[[3,"ti"]]'>senment: </span><span id="t1s_19" class="t sb_19" data-mappings='[[4,"ti"]]'>negave </span>
<span id="t1t_19" class="t s5_19">How to avoid </span>
<span id="t1u_19" class="t s5_19">catastrophic </span>
<span id="t1v_19" class="t s5_19" data-mappings='[[5,"tti"]]'>forgeng </span>
<span id="t1w_19" class="t s7_19" data-mappings='[[2,"ti"]]'>Opon1 </span>
<span id="t1x_19" class="t s6_19">• </span><span id="t1y_19" class="t s7_19" data-mappings='[[51,"tti"],[153,"fi"]]'>It's important to decide whether catastrophic forgeng actually impacts your use case.?. If all you need is  reliable performance on the single task you ﬁne-tuned on, </span><span id="t1z_19" class="t s7_19">it may not be an issue that the </span>
<span id="t20_19" class="t s7_19">model can't generalize to other tasks. </span>
<span id="t21_19" class="t s6_19">• </span><span id="t22_19" class="t s7_19" data-mappings='[[41,"ti"],[67,"ti"],[87,"fi"],[104,"ti"]]'>If you need the model to maintain its multask generalized capabilies you can perform ﬁne-tuning on mulple tasks. </span><span id="t23_19" class="t s5_19" data-mappings='[[3,"ti"]]'>Mul-task </span><span id="t24_19" class="t s7_19" data-mappings='[[0,"fi"]]'>ﬁne-tuning may require 50-100,000 examples across many tasks, and </span>
<span id="t25_19" class="t s7_19">so will require more data and compute to train e.g. FLAN </span><span id="t26_19" class="t se_19"> </span><span id="t27_19" class="t s7_19" data-mappings='[[29,"fl"],[31,"tt"],[67,"ti"]]'>FLAN-T5 and FLAN-PALM is the ﬂaening struct version of PALM foundaon model. </span>
<span id="t28_19" class="t s7_19" data-mappings='[[2,"ti"]]'>Opon2 </span>
<span id="t29_19" class="t s6_19">• </span>
<span id="t2a_19" class="t s7_19">Perform </span><span id="t2b_19" class="t s5_19">P</span><span id="t2c_19" class="t s7_19">arameter </span><span id="t2d_19" class="t s5_19">E</span><span id="t2e_19" class="t s7_19" data-mappings='[[0,"ffi"]]'>ﬃcient </span><span id="t2f_19" class="t s5_19">F</span><span id="t2g_19" class="t s7_19">ine</span><span id="t2h_19" class="t s5_19">-t</span><span id="t2i_19" class="t s7_19">uning (</span><span id="t2j_19" class="t s5_19">PEFT</span><span id="t2k_19" class="t s7_19" data-mappings='[[122,"fi"]]'>). PEFT is a set of techniques that preserves the weights of the original LLM and trains only a small number of task-speciﬁc adapter layers and parameters. </span>
<span id="t2l_19" class="t s6_19">• </span><span id="t2m_19" class="t s7_19" data-mappings='[[51,"tti"],[99,"ft"]]'>PEFT shows greater robustness to catastrophic forgeng since most of the pre-trained weights are leﬅ unchanged. </span>
<span id="t2n_19" class="t sf_19">Adapt and align model </span>
<span id="t2o_19" class="t sf_19">Fine-tuning </span>
<span id="t2p_19" class="t sg_19">Kumar </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
