<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p13" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_13{left:112px;bottom:745px;letter-spacing:0.13px;word-spacing:-0.07px;}
#t2_13{left:624px;bottom:745px;}
#t3_13{left:656px;bottom:745px;letter-spacing:0.16px;word-spacing:-0.07px;}
#t4_13{left:138px;bottom:20px;letter-spacing:0.1px;}
#t5_13{left:140px;bottom:642px;letter-spacing:-0.17px;word-spacing:0.15px;}
#t6_13{left:131px;bottom:628px;letter-spacing:-0.12px;word-spacing:0.1px;}
#t7_13{left:281px;bottom:640px;}
#t8_13{left:301px;bottom:642px;letter-spacing:-0.14px;word-spacing:0.02px;}
#t9_13{left:614px;bottom:642px;letter-spacing:-0.08px;word-spacing:-0.01px;}
#ta_13{left:281px;bottom:627px;}
#tb_13{left:301px;bottom:628px;letter-spacing:-0.15px;word-spacing:0.06px;}
#tc_13{left:999px;bottom:627px;letter-spacing:-0.2px;}
#td_13{left:1013px;bottom:614px;letter-spacing:-0.23px;}
#te_13{left:1105px;bottom:626px;letter-spacing:-0.18px;}
#tf_13{left:1113px;bottom:613px;letter-spacing:-0.31px;}
#tg_13{left:993px;bottom:673px;letter-spacing:0.09px;}
#th_13{left:1005px;bottom:658px;letter-spacing:0.18px;}
#ti_13{left:1096px;bottom:675px;letter-spacing:-0.01px;word-spacing:-0.08px;}
#tj_13{left:1093px;bottom:660px;letter-spacing:0.16px;word-spacing:0.09px;}
#tk_13{left:144px;bottom:558px;letter-spacing:-0.11px;word-spacing:-0.06px;}
#tl_13{left:285px;bottom:557px;}
#tm_13{left:305px;bottom:558px;letter-spacing:-0.15px;word-spacing:0.08px;}
#tn_13{left:285px;bottom:543px;}
#to_13{left:305px;bottom:545px;letter-spacing:-0.14px;word-spacing:0.04px;}
#tp_13{left:285px;bottom:530px;}
#tq_13{left:305px;bottom:531px;letter-spacing:-0.14px;word-spacing:0.01px;}
#tr_13{left:305px;bottom:517px;letter-spacing:-0.18px;word-spacing:0.08px;}
#ts_13{left:285px;bottom:502px;}
#tt_13{left:305px;bottom:503px;letter-spacing:-0.14px;word-spacing:0.08px;}
#tu_13{left:305px;bottom:490px;letter-spacing:-0.13px;word-spacing:0.06px;}
#tv_13{left:779px;bottom:490px;letter-spacing:-0.13px;word-spacing:-0.03px;}
#tw_13{left:1162px;bottom:561px;letter-spacing:0.01px;word-spacing:-0.02px;}
#tx_13{left:1162px;bottom:550px;letter-spacing:0.01px;}
#ty_13{left:1189px;bottom:550px;letter-spacing:0.05px;word-spacing:-0.05px;}
#tz_13{left:1162px;bottom:539px;letter-spacing:0.01px;}
#t10_13{left:1189px;bottom:539px;letter-spacing:0.09px;}
#t11_13{left:1162px;bottom:528px;letter-spacing:0.01px;}
#t12_13{left:1189px;bottom:528px;letter-spacing:0.15px;}
#t13_13{left:1162px;bottom:517px;letter-spacing:0.01px;}
#t14_13{left:1189px;bottom:517px;letter-spacing:0.04px;word-spacing:-0.05px;}
#t15_13{left:1162px;bottom:506px;letter-spacing:0.01px;}
#t16_13{left:1189px;bottom:506px;letter-spacing:-0.02px;}
#t17_13{left:1162px;bottom:495px;letter-spacing:0.01px;}
#t18_13{left:1189px;bottom:495px;}
#t19_13{left:1162px;bottom:484px;letter-spacing:0.01px;}
#t1a_13{left:1189px;bottom:484px;letter-spacing:0.05px;word-spacing:-0.05px;}
#t1b_13{left:1162px;bottom:473px;letter-spacing:0.01px;}
#t1c_13{left:1189px;bottom:473px;letter-spacing:0.17px;}
#t1d_13{left:136px;bottom:420px;letter-spacing:-0.1px;word-spacing:0.01px;}
#t1e_13{left:279px;bottom:418px;}
#t1f_13{left:299px;bottom:420px;letter-spacing:-0.13px;word-spacing:0.04px;}
#t1g_13{left:279px;bottom:404px;}
#t1h_13{left:299px;bottom:406px;letter-spacing:-0.16px;word-spacing:0.09px;}
#t1i_13{left:299px;bottom:392px;letter-spacing:-0.15px;word-spacing:0.02px;}
#t1j_13{left:130px;bottom:298px;letter-spacing:-0.13px;word-spacing:0.1px;}
#t1k_13{left:281px;bottom:298px;letter-spacing:-0.15px;word-spacing:0.1px;}
#t1l_13{left:281px;bottom:285px;letter-spacing:-0.13px;word-spacing:0.12px;}
#t1m_13{left:343px;bottom:285px;letter-spacing:-0.08px;}
#t1n_13{left:373px;bottom:285px;letter-spacing:-0.14px;word-spacing:0.05px;}
#t1o_13{left:281px;bottom:271px;letter-spacing:-0.15px;word-spacing:0.07px;}
#t1p_13{left:923px;bottom:177px;letter-spacing:-0.08px;word-spacing:-0.05px;}
#t1q_13{left:962px;bottom:177px;letter-spacing:-0.21px;}
#t1r_13{left:923px;bottom:160px;letter-spacing:-0.21px;word-spacing:0.19px;}
#t1s_13{left:925px;bottom:201px;letter-spacing:0.06px;word-spacing:0.01px;}
#t1t_13{left:1200px;bottom:259px;letter-spacing:-0.23px;}
#t1u_13{left:1073px;bottom:265px;letter-spacing:0.06px;}
#t1v_13{left:1104px;bottom:265px;letter-spacing:0.1px;}
#t1w_13{left:1084px;bottom:250px;letter-spacing:0.03px;word-spacing:0.05px;}
#t1x_13{left:1056px;bottom:291px;letter-spacing:0.22px;word-spacing:0.01px;}
#t1y_13{left:1275px;bottom:257px;letter-spacing:0.06px;}
#t1z_13{left:1307px;bottom:257px;letter-spacing:0.1px;}
#t20_13{left:1337px;bottom:257px;letter-spacing:0.03px;word-spacing:0.05px;}
#t21_13{left:1059px;bottom:206px;letter-spacing:0.22px;word-spacing:-0.15px;}
#t22_13{left:1205px;bottom:99px;letter-spacing:-0.31px;}
#t23_13{left:1059px;bottom:133px;letter-spacing:0.22px;}
#t24_13{left:1289px;bottom:96px;letter-spacing:0.03px;}
#t25_13{left:1312px;bottom:96px;letter-spacing:0.01px;word-spacing:0.07px;}
#t26_13{left:1358px;bottom:96px;letter-spacing:0.08px;}
#t27_13{left:132px;bottom:229px;letter-spacing:-0.12px;word-spacing:0.02px;}
#t28_13{left:281px;bottom:229px;letter-spacing:-0.14px;word-spacing:0.01px;}
#t29_13{left:742px;bottom:229px;letter-spacing:-0.12px;}
#t2a_13{left:767px;bottom:229px;letter-spacing:-0.1px;word-spacing:0.03px;}
#t2b_13{left:281px;bottom:215px;letter-spacing:-0.15px;word-spacing:0.06px;}
#t2c_13{left:281px;bottom:201px;letter-spacing:-0.14px;word-spacing:0.12px;}
#t2d_13{left:361px;bottom:201px;}
#t2e_13{left:368px;bottom:201px;letter-spacing:-0.18px;word-spacing:0.15px;}
#t2f_13{left:1079px;bottom:182px;letter-spacing:0.04px;word-spacing:0.03px;}
#t2g_13{left:1098px;bottom:168px;letter-spacing:0.1px;}
#t2h_13{left:1290px;bottom:175px;letter-spacing:0.02px;word-spacing:0.04px;}
#t2i_13{left:1351px;bottom:175px;letter-spacing:0.08px;}
#t2j_13{left:1203px;bottom:173px;letter-spacing:-0.23px;}
#t2k_13{left:1188px;bottom:295px;letter-spacing:0.22px;}
#t2l_13{left:1079px;bottom:105px;letter-spacing:0.04px;word-spacing:0.03px;}
#t2m_13{left:1084px;bottom:91px;letter-spacing:0.08px;}
#t2n_13{left:1104px;bottom:91px;letter-spacing:0.01px;}
#t2o_13{left:1308px;bottom:297px;letter-spacing:0.01px;}
#t2p_13{left:1190px;bottom:210px;letter-spacing:0.25px;}
#t2q_13{left:1310px;bottom:211px;letter-spacing:-0.02px;}
#t2r_13{left:1189px;bottom:135px;letter-spacing:0.19px;}
#t2s_13{left:1315px;bottom:135px;letter-spacing:-0.02px;}
#t2t_13{left:130px;bottom:156px;letter-spacing:-0.12px;word-spacing:-0.03px;}
#t2u_13{left:172px;bottom:142px;letter-spacing:-0.12px;}
#t2v_13{left:282px;bottom:156px;letter-spacing:-0.13px;}
#t2w_13{left:282px;bottom:142px;letter-spacing:-0.13px;word-spacing:0.03px;}
#t2x_13{left:282px;bottom:128px;letter-spacing:-0.15px;word-spacing:0.05px;}
#t2y_13{left:282px;bottom:114px;letter-spacing:-0.15px;word-spacing:0.14px;}
#t2z_13{left:515px;bottom:114px;letter-spacing:-0.11px;word-spacing:0.05px;}
#t30_13{left:144px;bottom:340px;letter-spacing:0.09px;word-spacing:0.05px;}
#t31_13{left:1274px;bottom:787px;letter-spacing:-0.09px;}
#t32_13{left:1200px;bottom:767px;letter-spacing:-0.08px;word-spacing:-0.01px;}

.s0_13{font-size:31px;font-family:DejaVuSans_6t;color:#000;}
.s1_13{font-size:18px;font-family:Carlito_77;color:#8B8B8B;}
.s2_13{font-size:14px;font-family:Carlito-Bold_7r;color:#000;}
.s3_13{font-size:14px;font-family:LiberationSans_6o;color:#000;}
.s4_13{font-size:14px;font-family:Carlito_77;color:#000;}
.s5_13{font-size:14px;font-family:Carlito-Italic_7c;color:#000;}
.s6_13{font-size:11px;font-family:Carlito_77;color:#000;}
.s7_13{font-size:12px;font-family:Carlito-Bold_7r;color:#000;}
.s8_13{font-size:14px;font-family:Carlito_77;color:#0563C1;}
.s9_13{font-size:9px;font-family:Carlito-Bold_7r;color:#000;}
.sa_13{font-size:9px;font-family:Carlito_77;color:#000;}
.sb_13{font-size:12px;font-family:Carlito_77;color:#000;}
.sc_13{font-size:12px;font-family:Carlito-BoldItalic_7h;color:#000;}
.sd_13{font-size:15px;font-family:Carlito-Bold_7r;color:#000;}
.se_13{font-size:17px;font-family:Carlito-Bold_7r;color:#000;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts13" type="text/css" >

@font-face {
	font-family: Carlito-BoldItalic_7h;
	src: url("fonts/Carlito-BoldItalic_7h.woff") format("woff");
}

@font-face {
	font-family: Carlito-Bold_7r;
	src: url("fonts/Carlito-Bold_7r.woff") format("woff");
}

@font-face {
	font-family: Carlito-Italic_7c;
	src: url("fonts/Carlito-Italic_7c.woff") format("woff");
}

@font-face {
	font-family: Carlito_77;
	src: url("fonts/Carlito_77.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_6t;
	src: url("fonts/DejaVuSans_6t.woff") format("woff");
}

@font-face {
	font-family: LiberationSans_6o;
	src: url("fonts/LiberationSans_6o.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg13Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg13" style="-webkit-user-select: none;"><object width="1466" height="825" data="13/13.svg" type="image/svg+xml" id="pdf13" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_13" class="t s0_13">Generative AI Project Life Cycle </span><span id="t2_13" class="t s0_13">- </span><span id="t3_13" class="t s0_13">Choose existing model or pre-train </span>
<span id="t4_13" class="t s1_13">Kumar </span>
<span id="t5_13" class="t s2_13" data-mappings='[[9,"ti"]]'>Consideraon for </span>
<span id="t6_13" class="t s2_13">choosing LLM model </span>
<span id="t7_13" class="t s3_13">• </span><span id="t8_13" class="t s4_13" data-mappings='[[5,"fi"],[45,"ti"]]'>Your ﬁrst choice will be to work with an exisng model. </span><span id="t9_13" class="t s5_13">(if it aligns to your governance and privacy polices) </span>
<span id="ta_13" class="t s3_13">• </span><span id="tb_13" class="t s4_13" data-mappings='[[15,"fi"]]'>There are speciﬁc circumstances where training your own model from scratch might be advantageous. </span>
<span id="tc_13" class="t s6_13">Pretrained </span>
<span id="td_13" class="t s6_13">LLM </span>
<span id="te_13" class="t s6_13">Custom </span>
<span id="tf_13" class="t s6_13">LLM </span>
<span id="tg_13" class="t s7_13" data-mappings='[[6,"ti"]]'>Foundaon </span>
<span id="th_13" class="t s7_13">model </span>
<span id="ti_13" class="t s7_13">Train your </span>
<span id="tj_13" class="t s7_13">own model </span>
<span id="tk_13" class="t s2_13">LLM model hubs </span><span id="tl_13" class="t s3_13">• </span><span id="tm_13" class="t s4_13" data-mappings='[[62,"ti"],[79,"ti"],[89,"ti"]]'>In general you'll begin the process of developing your applicaon using an exisng foundaon model. </span>
<span id="tn_13" class="t s3_13">• </span><span id="to_13" class="t s4_13" data-mappings='[[55,"ti"]]'>Many open-source models are available to use in applicaon. </span>
<span id="tp_13" class="t s3_13">• </span><span id="tq_13" class="t s4_13" data-mappings='[[66,"ti"],[80,"ti"]]'>The developers of some of the major frameworks for building generave AI applicaons like Hugging Face and </span>
<span id="tr_13" class="t s4_13">PyTorch , have curated hubs where you can browse these models. </span>
<span id="ts_13" class="t s3_13">• </span><span id="tt_13" class="t s4_13">A really useful feature of these hubs is the inclusion of model cards, that describe important details including the </span>
<span id="tu_13" class="t s4_13" data-mappings='[[67,"ti"]]'>best use cases for each model, how it was trained, and known limitaons of LLM e.g. </span><span id="tv_13" class="t s8_13">Model Card for FLAN-T5 base </span>
<span id="tw_13" class="t s9_13">Table of content </span>
<span id="tx_13" class="t sa_13">1. </span><span id="ty_13" class="t sa_13">Model Details </span>
<span id="tz_13" class="t sa_13">2. </span><span id="t10_13" class="t sa_13">Usage </span>
<span id="t11_13" class="t sa_13">3. </span><span id="t12_13" class="t sa_13">Uses </span>
<span id="t13_13" class="t sa_13">4. </span><span id="t14_13" class="t sa_13" data-mappings='[[22,"ti"]]'>Bias, Risks and Limitaon </span>
<span id="t15_13" class="t sa_13">5. </span><span id="t16_13" class="t sa_13">Training Details </span>
<span id="t17_13" class="t sa_13">6. </span><span id="t18_13" class="t sa_13" data-mappings='[[6,"ti"]]'>Evaluaon </span>
<span id="t19_13" class="t sa_13">7. </span><span id="t1a_13" class="t sa_13">Environmental Impact </span>
<span id="t1b_13" class="t sa_13">8. </span><span id="t1c_13" class="t sa_13">….. </span>
<span id="t1d_13" class="t s2_13">LLM models types </span><span id="t1e_13" class="t s3_13">• </span><span id="t1f_13" class="t s4_13">The exact model that you'd choose will depend on the details of the task, you need to carry out. </span>
<span id="t1g_13" class="t s3_13">• </span><span id="t1h_13" class="t s4_13" data-mappings='[[63,"ff"]]'>Variance of the transformer model architecture are suited to diﬀerent language tasks, largely because of </span>
<span id="t1i_13" class="t s4_13" data-mappings='[[2,"ff"]]'>diﬀerences in how the models are trained. </span>
<span id="t1j_13" class="t s2_13">Encoder LLM models </span><span id="t1k_13" class="t s4_13">Encoder-only models are also known as Autoencoding models and are pre-trained using masked language </span>
<span id="t1l_13" class="t s4_13">modelling (</span><span id="t1m_13" class="t s2_13">MLM</span><span id="t1n_13" class="t s4_13" data-mappings='[[89,"ft"]]'>). They correspond to the encoder part of the original transformer architecture and are oﬅen </span>
<span id="t1o_13" class="t s4_13" data-mappings='[[25,"fi"],[28,"ti"],[47,"fi"],[50,"ti"]]'>used with sentence classiﬁcaon or token classiﬁcaon e.g. BERT and RoBERTa </span>
<span id="t1p_13" class="t s4_13">It was </span><span id="t1q_13" class="t s4_13">great </span>
<span id="t1r_13" class="t s4_13">day today </span>
<span id="t1s_13" class="t s7_13">Original text </span>
<span id="t1t_13" class="t s6_13">LLM </span>
<span id="t1u_13" class="t sb_13">It was </span><span id="t1v_13" class="t sc_13">&lt;mask&gt; </span>
<span id="t1w_13" class="t sb_13">day today </span>
<span id="t1x_13" class="t s6_13">Autoencoding : MLM </span>
<span id="t1y_13" class="t sb_13">It was </span><span id="t1z_13" class="t sc_13">great </span><span id="t20_13" class="t sb_13">day today </span>
<span id="t21_13" class="t s6_13">Autoencoding : CLM </span>
<span id="t22_13" class="t s6_13">LLM </span>
<span id="t23_13" class="t s6_13">Sequence-to-sequence </span>
<span id="t24_13" class="t sb_13">&lt;x&gt; </span><span id="t25_13" class="t sb_13">great day</span><span id="t26_13" class="t sb_13">.... </span>
<span id="t27_13" class="t s2_13">Decoder LLM models </span><span id="t28_13" class="t s4_13">Decoder (Autoregressive) models are pre-trained using causal language modelling (</span><span id="t29_13" class="t s2_13">CLM</span><span id="t2a_13" class="t s4_13">). Models of this type </span>
<span id="t2b_13" class="t s4_13" data-mappings='[[81,"ft"]]'>make use of the decoder component of the original transformer architecture, and oﬅen used for text </span>
<span id="t2c_13" class="t s4_13" data-mappings='[[6,"ti"]]'>generaon e.g</span><span id="t2d_13" class="t s2_13">. </span><span id="t2e_13" class="t s4_13">GBT and BLOOM </span>
<span id="t2f_13" class="t sb_13">It was great </span>
<span id="t2g_13" class="t sc_13">&lt;?&gt; </span>
<span id="t2h_13" class="t sb_13">It was great </span><span id="t2i_13" class="t sc_13">day </span>
<span id="t2j_13" class="t s6_13">LLM </span>
<span id="t2k_13" class="t s6_13">Encoder </span>
<span id="t2l_13" class="t sb_13">It was great </span>
<span id="t2m_13" class="t sc_13">&lt;?&gt; </span><span id="t2n_13" class="t sb_13">today </span>
<span id="t2o_13" class="t s6_13">Target </span>
<span id="t2p_13" class="t s6_13">Decoder </span>
<span id="t2q_13" class="t s6_13">Target </span>
<span id="t2r_13" class="t s6_13">Encoder-Decoder </span><span id="t2s_13" class="t s6_13">Target </span>
<span id="t2t_13" class="t s2_13">Encoder-Decoder LLM </span>
<span id="t2u_13" class="t s2_13">models </span>
<span id="t2v_13" class="t s4_13" data-mappings='[[67,"ff"]]'>Sequence-to-sequence models use both the encoder and decoder part oﬀ the original transformer </span>
<span id="t2w_13" class="t s4_13" data-mappings='[[57,"ti"]]'>architecture. The exact details of the pre-training objecve vary from model to model. For e.g., T5 model is </span>
<span id="t2x_13" class="t s4_13" data-mappings='[[29,"ti"],[67,"ft"],[87,"ti"]]'>pre-trained using span corrupon. Sequence-to-sequence models are oﬅen used for translaon, </span>
<span id="t2y_13" class="t s4_13" data-mappings='[[9,"ti"],[22,"ti"]]'>summarizaon, and queson-answering. </span><span id="t2z_13" class="t s4_13">e.g. T5 , BART </span>
<span id="t30_13" class="t sd_13">Pre-train LLM model by transformer type </span>
<span id="t31_13" class="t se_13">Select </span>
<span id="t32_13" class="t se_13" data-mappings='[[4,"ti"]]'>Exisng models or pre-train </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
