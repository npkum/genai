<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p24" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_24{left:112px;bottom:735px;letter-spacing:-0.05px;word-spacing:0.12px;}
#t2_24{left:619px;bottom:735px;letter-spacing:-0.07px;word-spacing:0.08px;}
#t3_24{left:76px;bottom:15px;letter-spacing:0.13px;}
#t4_24{left:1186px;bottom:781px;letter-spacing:-0.09px;word-spacing:0.05px;}
#t5_24{left:1170px;bottom:761px;letter-spacing:-0.09px;}
#t6_24{left:91px;bottom:694px;letter-spacing:0.11px;word-spacing:0.07px;}
#t7_24{left:91px;bottom:672px;letter-spacing:0.14px;word-spacing:0.05px;}
#t8_24{left:91px;bottom:653px;}
#t9_24{left:112px;bottom:650px;letter-spacing:0.1px;word-spacing:0.1px;}
#ta_24{left:113px;bottom:591px;}
#tb_24{left:151px;bottom:591px;letter-spacing:-0.16px;word-spacing:0.03px;}
#tc_24{left:205px;bottom:591px;letter-spacing:-0.14px;word-spacing:0.06px;}
#td_24{left:94px;bottom:618px;letter-spacing:0.09px;word-spacing:0.04px;}
#te_24{left:113px;bottom:564px;}
#tf_24{left:151px;bottom:564px;letter-spacing:-0.14px;word-spacing:0.03px;}
#tg_24{left:113px;bottom:537px;}
#th_24{left:151px;bottom:537px;letter-spacing:-0.15px;word-spacing:0.05px;}
#ti_24{left:151px;bottom:523px;letter-spacing:-0.14px;word-spacing:0.04px;}
#tj_24{left:113px;bottom:499px;}
#tk_24{left:151px;bottom:497px;}
#tl_24{left:172px;bottom:499px;letter-spacing:-0.14px;word-spacing:0.04px;}
#tm_24{left:172px;bottom:485px;letter-spacing:-0.17px;word-spacing:0.08px;}
#tn_24{left:151px;bottom:470px;}
#to_24{left:172px;bottom:471px;letter-spacing:-0.13px;word-spacing:0.01px;}
#tp_24{left:1070px;bottom:532px;letter-spacing:-0.12px;}
#tq_24{left:1070px;bottom:520px;letter-spacing:-0.11px;}
#tr_24{left:1224px;bottom:489px;letter-spacing:0.11px;}
#ts_24{left:1226px;bottom:479px;letter-spacing:0.2px;}
#tt_24{left:1262px;bottom:489px;letter-spacing:0.09px;}
#tu_24{left:1304px;bottom:533px;letter-spacing:-0.08px;}
#tv_24{left:1289px;bottom:522px;letter-spacing:-0.08px;}
#tw_24{left:112px;bottom:434px;}
#tx_24{left:151px;bottom:434px;letter-spacing:-0.13px;word-spacing:0.04px;}
#ty_24{left:1152px;bottom:454px;letter-spacing:0.09px;word-spacing:-0.1px;}
#tz_24{left:1273px;bottom:454px;letter-spacing:0.09px;word-spacing:0.03px;}
#t10_24{left:112px;bottom:408px;}
#t11_24{left:151px;bottom:408px;letter-spacing:-0.14px;word-spacing:0.05px;}
#t12_24{left:151px;bottom:394px;letter-spacing:-0.16px;word-spacing:0.01px;}
#t13_24{left:1263px;bottom:510px;letter-spacing:0.04px;}
#t14_24{left:112px;bottom:371px;}
#t15_24{left:151px;bottom:371px;letter-spacing:-0.14px;word-spacing:0.04px;}
#t16_24{left:151px;bottom:357px;letter-spacing:-0.15px;word-spacing:0.12px;}
#t17_24{left:266px;bottom:357px;letter-spacing:-0.12px;word-spacing:0.02px;}
#t18_24{left:378px;bottom:357px;}
#t19_24{left:112px;bottom:328px;}
#t1a_24{left:151px;bottom:328px;letter-spacing:-0.15px;word-spacing:0.04px;}
#t1b_24{left:151px;bottom:315px;letter-spacing:-0.11px;word-spacing:-0.01px;}
#t1c_24{left:319px;bottom:315px;}
#t1d_24{left:326px;bottom:315px;letter-spacing:-0.18px;}
#t1e_24{left:371px;bottom:315px;}
#t1f_24{left:378px;bottom:315px;letter-spacing:-0.07px;}
#t1g_24{left:407px;bottom:315px;}
#t1h_24{left:416px;bottom:315px;letter-spacing:-0.17px;word-spacing:0.14px;}
#t1i_24{left:486px;bottom:315px;letter-spacing:-0.13px;}
#t1j_24{left:510px;bottom:315px;letter-spacing:-0.1px;word-spacing:0.12px;}
#t1k_24{left:1301px;bottom:551px;letter-spacing:0.1px;}
#t1l_24{left:112px;bottom:254px;}
#t1m_24{left:151px;bottom:254px;letter-spacing:-0.17px;word-spacing:0.08px;}
#t1n_24{left:151px;bottom:240px;letter-spacing:-0.12px;word-spacing:0.01px;}
#t1o_24{left:154px;bottom:226px;letter-spacing:-0.14px;word-spacing:0.09px;}
#t1p_24{left:277px;bottom:226px;letter-spacing:-0.2px;word-spacing:0.17px;}
#t1q_24{left:368px;bottom:226px;letter-spacing:-0.16px;word-spacing:0.06px;}
#t1r_24{left:872px;bottom:226px;letter-spacing:-0.15px;word-spacing:-0.02px;}
#t1s_24{left:956px;bottom:226px;letter-spacing:-1.37px;}
#t1t_24{left:151px;bottom:212px;letter-spacing:-0.15px;word-spacing:0.05px;}
#t1u_24{left:94px;bottom:279px;letter-spacing:0.06px;word-spacing:0.11px;}
#t1v_24{left:109px;bottom:183px;letter-spacing:-0.06px;}
#t1w_24{left:151px;bottom:181px;}
#t1x_24{left:172px;bottom:183px;letter-spacing:-0.31px;word-spacing:0.16px;}
#t1y_24{left:237px;bottom:183px;letter-spacing:-0.14px;word-spacing:0.04px;}
#t1z_24{left:172px;bottom:169px;letter-spacing:-0.15px;word-spacing:0.04px;}
#t20_24{left:151px;bottom:154px;}
#t21_24{left:172px;bottom:155px;letter-spacing:-0.15px;word-spacing:0.04px;}
#t22_24{left:172px;bottom:142px;letter-spacing:-0.16px;word-spacing:0.06px;}
#t23_24{left:1071px;bottom:226px;letter-spacing:-0.12px;}
#t24_24{left:1071px;bottom:214px;letter-spacing:-0.11px;}
#t25_24{left:1152px;bottom:227px;letter-spacing:-0.15px;}
#t26_24{left:1159px;bottom:215px;letter-spacing:-0.09px;}
#t27_24{left:1232px;bottom:227px;letter-spacing:0.03px;}
#t28_24{left:1244px;bottom:215px;letter-spacing:-0.02px;}
#t29_24{left:109px;bottom:112px;letter-spacing:-0.06px;}
#t2a_24{left:151px;bottom:111px;}
#t2b_24{left:172px;bottom:112px;letter-spacing:-0.15px;word-spacing:0.05px;}
#t2c_24{left:797px;bottom:112px;letter-spacing:-0.13px;word-spacing:-0.02px;}
#t2d_24{left:876px;bottom:112px;}
#t2e_24{left:1186px;bottom:163px;letter-spacing:0.04px;word-spacing:-0.05px;}
#t2f_24{left:1189px;bottom:152px;letter-spacing:0.05px;word-spacing:-0.06px;}
#t2g_24{left:1304px;bottom:182px;letter-spacing:0.11px;}
#t2h_24{left:1306px;bottom:172px;letter-spacing:0.2px;}
#t2i_24{left:1355px;bottom:234px;letter-spacing:-0.06px;}
#t2j_24{left:109px;bottom:87px;letter-spacing:-0.06px;}
#t2k_24{left:151px;bottom:85px;}
#t2l_24{left:172px;bottom:87px;letter-spacing:-0.15px;word-spacing:0.05px;}
#t2m_24{left:151px;bottom:72px;}
#t2n_24{left:172px;bottom:73px;letter-spacing:-0.15px;word-spacing:0.04px;}
#t2o_24{left:1157px;bottom:540px;letter-spacing:0.11px;}
#t2p_24{left:1164px;bottom:530px;letter-spacing:0.21px;}
#t2q_24{left:1150px;bottom:519px;letter-spacing:0.2px;word-spacing:-0.08px;}
#t2r_24{left:1213px;bottom:534px;letter-spacing:0.21px;word-spacing:-0.08px;}

.s0_24{font-size:31px;font-family:DejaVuSans_6t;color:#000;}
.s1_24{font-size:18px;font-family:Carlito_77;color:#8B8B8B;}
.s2_24{font-size:17px;font-family:Carlito-Bold_7r;color:#000;}
.s3_24{font-size:18px;font-family:Carlito_77;color:#000;}
.s4_24{font-size:18px;font-family:OpenSymbol_7m;color:#000;}
.s5_24{font-size:14px;font-family:Carlito_77;color:#000;}
.s6_24{font-size:15px;font-family:Carlito-Bold_7r;color:#000;}
.s7_24{font-size:14px;font-family:LiberationSans_6o;color:#000;}
.s8_24{font-size:10px;font-family:Carlito_77;color:#000;}
.s9_24{font-size:8px;font-family:Carlito_77;color:#000;}
.sa_24{font-size:9px;font-family:Carlito_77;color:#000;}
.sb_24{font-size:9px;font-family:Carlito-Italic_7c;color:#000;}
.sc_24{font-size:14px;font-family:Carlito-Bold_7r;color:#000;}
.sd_24{font-size:14px;font-family:Carlito-Italic_7c;color:#000;}
.se_24{font-size:8px;font-family:Carlito-Italic_7c;color:#000;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts24" type="text/css" >

@font-face {
	font-family: Carlito-Bold_7r;
	src: url("fonts/Carlito-Bold_7r.woff") format("woff");
}

@font-face {
	font-family: Carlito-Italic_7c;
	src: url("fonts/Carlito-Italic_7c.woff") format("woff");
}

@font-face {
	font-family: Carlito_77;
	src: url("fonts/Carlito_77.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_6t;
	src: url("fonts/DejaVuSans_6t.woff") format("woff");
}

@font-face {
	font-family: LiberationSans_6o;
	src: url("fonts/LiberationSans_6o.woff") format("woff");
}

@font-face {
	font-family: OpenSymbol_7m;
	src: url("fonts/OpenSymbol_7m.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg24Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg24" style="-webkit-user-select: none;"><object width="1466" height="825" data="24/24.svg" type="image/svg+xml" id="pdf24" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_24" class="t s0_24">Generative AI Project Life Cycle </span><span id="t2_24" class="t s0_24">- Reinforcement learning for human feedback </span>
<span id="t3_24" class="t s1_24">Kumar </span>
<span id="t4_24" class="t s2_24">Adapt and align model </span>
<span id="t5_24" class="t s2_24">Align with human feedback </span>
<span id="t6_24" class="t s3_24">How the reward model gets used in the reinforcement learning process to train your human aligned LLM. How does the reward model in the reinforcement learning </span>
<span id="t7_24" class="t s3_24">process updates the LLM weights and produce a human aligned model. </span>
<span id="t8_24" class="t s4_24"> </span>
<span id="t9_24" class="t s3_24">Start with a model that already has good performance on your task of interests. </span>
<span id="ta_24" class="t s5_24">1 </span><span id="tb_24" class="t s5_24">First you </span><span id="tc_24" class="t s5_24" data-mappings='[[89,"ti"]]'>pass a prompt from your prompt dataset to the instruct LLM, which then generates a compleon. </span>
<span id="td_24" class="t s6_24">Overview of Fine-tunning with reinforcement learning for RLHF </span>
<span id="te_24" class="t s5_24">2 </span><span id="tf_24" class="t s5_24" data-mappings='[[26,"ti"],[95,"ti"]]'>Next, you send this compleon, and the original prompt to the reward model as the prompt compleon pair to reward model. </span>
<span id="tg_24" class="t s5_24">3 </span><span id="th_24" class="t s5_24">The reward model evaluates the pair based on the human feedback it was trained on and returns a reward value. A higher reward value for e.g., 0.15 as </span>
<span id="ti_24" class="t s5_24" data-mappings='[[112,"ti"]]'>shown here represents a more aligned response. A less aligned response would receive a lower value, such as negave -0.43 as an e.g. </span>
<span id="tj_24" class="t s5_24">4 </span><span id="tk_24" class="t s7_24">• </span><span id="tl_24" class="t s5_24" data-mappings='[[49,"ti"]]'>Then pass this reward value for the prompt compleon pair to the reinforcement learning algorithm to update the weights of the LLM, and move it </span>
<span id="tm_24" class="t s5_24" data-mappings='[[14,"ti"]]'>towards generang more aligned, higher reward responses. </span>
<span id="tn_24" class="t s7_24">• </span><span id="to_24" class="t s5_24">The reinforcement learning (RL) algorithm updates weights of the model. The Instruct LLM now has updated weights. </span>
<span id="tp_24" class="t s8_24">Prompt </span>
<span id="tq_24" class="t s8_24">dataset </span>
<span id="tr_24" class="t s9_24">Reward </span>
<span id="ts_24" class="t s9_24">Model </span>
<span id="tt_24" class="t sa_24">0.15 </span>
<span id="tu_24" class="t s8_24">RL </span>
<span id="tv_24" class="t s8_24">algorithm </span>
<span id="tw_24" class="t s5_24">5 </span><span id="tx_24" class="t s5_24" data-mappings='[[11,"ti"],[19,"ti"],[80,"fi"]]'>These iteraons connue for a given number of epochs, similar to other types of ﬁne tuning. </span>
<span id="ty_24" class="t sb_24" data-mappings='[[5,"ti"]]'>Iteraon 1 </span><span id="tz_24" class="t sb_24" data-mappings='[[5,"ti"]]'>Iteraon.. n </span>
<span id="t10_24" class="t s5_24">6 </span><span id="t11_24" class="t s5_24" data-mappings='[[84,"ft"],[98,"ti"]]'>If the process is working well, you'll see the reward improving i.e., higher score aﬅer each iteraon as the model produces text that is increasingly aligned </span>
<span id="t12_24" class="t s5_24">with human preferences. </span>
<span id="t13_24" class="t sa_24">0.53 </span>
<span id="t14_24" class="t s5_24">7 </span><span id="t15_24" class="t s5_24" data-mappings='[[12,"ti"],[27,"ti"],[41,"ti"],[86,"ti"],[110,"ti"],[112,"fi"],[122,"ti"]]'>You will connue this iterave process unl your model is aligned based on some evaluaon criteria. Once it sasﬁes evaluaon metrics you can refer the </span>
<span id="t16_24" class="t s5_24" data-mappings='[[0,"fi"]]'>ﬁne-tuned model as </span><span id="t17_24" class="t sc_24">human aligned LLM</span><span id="t18_24" class="t s5_24">. </span>
<span id="t19_24" class="t s5_24">8 </span><span id="t1a_24" class="t s5_24">There are several algorithms that takes the output of the reward model and uses it to update the LLM model weights so that the reward score increases </span>
<span id="t1b_24" class="t s5_24" data-mappings='[[5,"ti"]]'>over me. A popular choice is </span><span id="t1c_24" class="t sc_24">P</span><span id="t1d_24" class="t s5_24">roximal </span><span id="t1e_24" class="t sc_24">P</span><span id="t1f_24" class="t s5_24">olicy </span><span id="t1g_24" class="t sc_24">O</span><span id="t1h_24" class="t s5_24" data-mappings='[[1,"ti"],[6,"ti"]]'>pmizaon (</span><span id="t1i_24" class="t sc_24">PPO</span><span id="t1j_24" class="t s5_24">) . </span>
<span id="t1k_24" class="t sa_24">PPO </span>
<span id="t1l_24" class="t s5_24">9 </span><span id="t1m_24" class="t s5_24" data-mappings='[[64,"ti"],[109,"ti"]]'>However, in reinforcement learning where the algorithm can potenal learn to cheat the system by favouring acons that maximize the reward received </span>
<span id="t1n_24" class="t s5_24" data-mappings='[[16,"ti"],[61,"ti"]]'>even if those acons don't align well with the original objecve. </span>
<span id="t1o_24" class="t s5_24">This problem is called </span><span id="t1p_24" class="t sc_24">Reward Hacking</span><span id="t1q_24" class="t s5_24" data-mappings='[[41,"ti"]]'>. For example, the model can start generang low toxicity scores by including phrases like “</span><span id="t1r_24" class="t sd_24">most awesome</span><span id="t1s_24" class="t s5_24">”, </span>
<span id="t1t_24" class="t s5_24" data-mappings='[[73,"fi"]]'>“incredible” or “reliable” these phrase may sound exaggerated for a speciﬁc task. </span>
<span id="t1u_24" class="t s6_24">Reward Hacking and How to avoid reward hacking </span>
<span id="t1v_24" class="t s5_24">10 </span><span id="t1w_24" class="t s7_24">• </span><span id="t1x_24" class="t s5_24">To prevent </span><span id="t1y_24" class="t s5_24" data-mappings='[[50,"ti"]]'>reward hacking from happening, you can use the inial instruct LLM as performance reference (reference model). The weights of the </span>
<span id="t1z_24" class="t s5_24" data-mappings='[[59,"ti"]]'>reference model are frozen and are not updated during iteraons of reinforcement learning (RL). </span>
<span id="t20_24" class="t s7_24">• </span><span id="t21_24" class="t s5_24" data-mappings='[[130,"ti"],[142,"ti"]]'>This way, you always maintain a single reference model to compare to during training, each prompt is passed to both models, generang a compleon </span>
<span id="t22_24" class="t s5_24">by the reference LLM and the intermediate LLM model is updated. </span>
<span id="t23_24" class="t s8_24">Prompt </span>
<span id="t24_24" class="t s8_24">dataset </span>
<span id="t25_24" class="t s8_24">Reference </span>
<span id="t26_24" class="t s8_24">Model </span>
<span id="t27_24" class="t sa_24">RL-updated </span>
<span id="t28_24" class="t s8_24">LLM </span>
<span id="t29_24" class="t s5_24">11 </span><span id="t2a_24" class="t s7_24">• </span><span id="t2b_24" class="t s5_24" data-mappings='[[45,"ti"]]'>At this point, you can compare the two compleons and calculate a value called the Kullback-Leibler divergence (</span><span id="t2c_24" class="t sc_24">KL divergence</span><span id="t2d_24" class="t s5_24">) </span>
<span id="t2e_24" class="t sa_24">KL Divergence </span>
<span id="t2f_24" class="t sa_24" data-mappings='[[3,"ft"]]'>Shiﬅ Penalty </span>
<span id="t2g_24" class="t s9_24">Reward </span>
<span id="t2h_24" class="t s9_24">Model </span>
<span id="t2i_24" class="t s8_24">PPO </span>
<span id="t2j_24" class="t s5_24">12 </span><span id="t2k_24" class="t s7_24">• </span><span id="t2l_24" class="t s5_24" data-mappings='[[97,"ti"]]'>Once KL divergence is calculated between the two models, you can added them to the reward calculaon. </span>
<span id="t2m_24" class="t s7_24">• </span><span id="t2n_24" class="t s5_24" data-mappings='[[49,"ft"],[103,"ti"],[123,"ff"]]'>This will penalize the RL updated model if it shiﬅs too far from the reference LLM and generates compleons that are two diﬀerent. </span>
<span id="t2o_24" class="t s9_24">Instruct </span>
<span id="t2p_24" class="t s9_24">LLM </span>
<span id="t2q_24" class="t s9_24">RL Updated </span>
<span id="t2r_24" class="t se_24">update LLM </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
