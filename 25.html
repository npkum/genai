<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p25" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_25{left:112px;bottom:736px;letter-spacing:-0.12px;word-spacing:0.09px;}
#t2_25{left:601px;bottom:736px;letter-spacing:-0.1px;word-spacing:0.05px;}
#t3_25{left:1185px;bottom:781px;letter-spacing:-0.1px;word-spacing:0.01px;}
#t4_25{left:1220px;bottom:761px;letter-spacing:-0.12px;}
#t5_25{left:91px;bottom:694px;letter-spacing:0.1px;word-spacing:0.11px;}
#t6_25{left:91px;bottom:673px;}
#t7_25{left:112px;bottom:672px;letter-spacing:0.14px;word-spacing:0.04px;}
#t8_25{left:91px;bottom:651px;}
#t9_25{left:112px;bottom:650px;letter-spacing:0.12px;word-spacing:0.05px;}
#ta_25{left:91px;bottom:629px;}
#tb_25{left:112px;bottom:628px;letter-spacing:0.1px;word-spacing:0.11px;}
#tc_25{left:91px;bottom:607px;}
#td_25{left:112px;bottom:606px;letter-spacing:0.11px;word-spacing:0.04px;}
#te_25{left:86px;bottom:558px;letter-spacing:0.08px;word-spacing:0.09px;}
#tf_25{left:169px;bottom:558px;letter-spacing:0.11px;word-spacing:0.07px;}
#tg_25{left:98px;bottom:527px;letter-spacing:-0.01px;}
#th_25{left:204px;bottom:527px;letter-spacing:0.01px;}
#ti_25{left:98px;bottom:500px;letter-spacing:-0.17px;}
#tj_25{left:204px;bottom:500px;letter-spacing:-0.14px;word-spacing:0.05px;}
#tk_25{left:509px;bottom:500px;letter-spacing:-0.14px;word-spacing:-0.01px;}
#tl_25{left:601px;bottom:500px;letter-spacing:-0.12px;word-spacing:0.04px;}
#tm_25{left:787px;bottom:500px;letter-spacing:-0.13px;}
#tn_25{left:832px;bottom:500px;letter-spacing:-0.15px;word-spacing:0.05px;}
#to_25{left:204px;bottom:486px;letter-spacing:-0.16px;word-spacing:0.05px;}
#tp_25{left:204px;bottom:471px;}
#tq_25{left:225px;bottom:472px;letter-spacing:-0.17px;word-spacing:0.05px;}
#tr_25{left:204px;bottom:457px;}
#ts_25{left:225px;bottom:458px;letter-spacing:-0.16px;word-spacing:0.09px;}
#tt_25{left:204px;bottom:443px;}
#tu_25{left:225px;bottom:445px;letter-spacing:-0.16px;word-spacing:0.08px;}
#tv_25{left:204px;bottom:429px;}
#tw_25{left:225px;bottom:431px;letter-spacing:-0.14px;word-spacing:0.04px;}
#tx_25{left:1185px;bottom:458px;letter-spacing:0.05px;}
#ty_25{left:1178px;bottom:447px;letter-spacing:-0.04px;}
#tz_25{left:1302px;bottom:460px;letter-spacing:0.21px;}
#t10_25{left:1296px;bottom:450px;letter-spacing:0.15px;}
#t11_25{left:1177px;bottom:497px;letter-spacing:-0.11px;}
#t12_25{left:1229px;bottom:459px;letter-spacing:0.03px;}
#t13_25{left:1231px;bottom:448px;}
#t14_25{left:1182px;bottom:533px;letter-spacing:-0.05px;}
#t15_25{left:1293px;bottom:497px;letter-spacing:-0.11px;}
#t16_25{left:1239px;bottom:402px;letter-spacing:-0.17px;}
#t17_25{left:1240px;bottom:390px;letter-spacing:-0.11px;}
#t18_25{left:1234px;bottom:540px;letter-spacing:0.03px;}
#t19_25{left:1235px;bottom:529px;letter-spacing:0.01px;}
#t1a_25{left:1287px;bottom:535px;letter-spacing:-0.11px;}
#t1b_25{left:101px;bottom:349px;letter-spacing:-0.17px;}
#t1c_25{left:207px;bottom:349px;letter-spacing:-0.16px;word-spacing:0.1px;}
#t1d_25{left:520px;bottom:349px;}
#t1e_25{left:527px;bottom:349px;letter-spacing:-0.28px;}
#t1f_25{left:548px;bottom:349px;}
#t1g_25{left:555px;bottom:349px;letter-spacing:-0.16px;}
#t1h_25{left:596px;bottom:349px;}
#t1i_25{left:606px;bottom:349px;letter-spacing:-0.16px;}
#t1j_25{left:675px;bottom:349px;letter-spacing:-0.32px;}
#t1k_25{left:698px;bottom:349px;letter-spacing:-0.13px;word-spacing:-0.01px;}
#t1l_25{left:207px;bottom:334px;}
#t1m_25{left:227px;bottom:336px;letter-spacing:-0.16px;word-spacing:0.08px;}
#t1n_25{left:227px;bottom:322px;letter-spacing:-0.13px;}
#t1o_25{left:207px;bottom:307px;}
#t1p_25{left:227px;bottom:308px;letter-spacing:-0.15px;word-spacing:0.05px;}
#t1q_25{left:227px;bottom:294px;letter-spacing:-0.14px;word-spacing:0.02px;}
#t1r_25{left:1127px;bottom:348px;letter-spacing:-0.11px;}
#t1s_25{left:1159px;bottom:348px;letter-spacing:-0.14px;}
#t1t_25{left:1210px;bottom:348px;letter-spacing:-0.2px;}
#t1u_25{left:1265px;bottom:352px;letter-spacing:-0.14px;}
#t1v_25{left:1253px;bottom:344px;letter-spacing:-0.15px;word-spacing:-0.04px;}
#t1w_25{left:1337px;bottom:352px;letter-spacing:-0.11px;word-spacing:-0.23px;}
#t1x_25{left:1346px;bottom:344px;letter-spacing:-0.16px;}
#t1y_25{left:1080px;bottom:328px;letter-spacing:-0.18px;}
#t1z_25{left:1127px;bottom:328px;letter-spacing:-0.24px;}
#t20_25{left:1159px;bottom:328px;}
#t21_25{left:1210px;bottom:328px;letter-spacing:-0.24px;}
#t22_25{left:1251px;bottom:328px;letter-spacing:-0.1px;word-spacing:-0.26px;}
#t23_25{left:1321px;bottom:328px;letter-spacing:-0.2px;}
#t24_25{left:1080px;bottom:312px;letter-spacing:-0.18px;}
#t25_25{left:1127px;bottom:312px;letter-spacing:-0.24px;}
#t26_25{left:1159px;bottom:312px;}
#t27_25{left:1210px;bottom:312px;letter-spacing:-0.24px;}
#t28_25{left:1251px;bottom:312px;letter-spacing:-0.1px;word-spacing:-0.26px;}
#t29_25{left:1321px;bottom:312px;letter-spacing:-0.17px;}
#t2a_25{left:1080px;bottom:296px;letter-spacing:-0.27px;}
#t2b_25{left:1127px;bottom:296px;letter-spacing:-0.24px;}
#t2c_25{left:1159px;bottom:296px;}
#t2d_25{left:1210px;bottom:296px;}
#t2e_25{left:1251px;bottom:296px;letter-spacing:-0.1px;word-spacing:-0.26px;}
#t2f_25{left:1321px;bottom:296px;letter-spacing:-0.17px;}
#t2g_25{left:1080px;bottom:280px;letter-spacing:-0.14px;}
#t2h_25{left:1127px;bottom:280px;}
#t2i_25{left:1159px;bottom:280px;letter-spacing:-0.1px;}
#t2j_25{left:1210px;bottom:280px;}
#t2k_25{left:1251px;bottom:280px;letter-spacing:-0.12px;word-spacing:-0.24px;}
#t2l_25{left:123px;bottom:229px;letter-spacing:-0.09px;}
#t2m_25{left:120px;bottom:215px;letter-spacing:-0.13px;}
#t2n_25{left:206px;bottom:229px;letter-spacing:-0.15px;word-spacing:0.08px;}
#t2o_25{left:206px;bottom:215px;letter-spacing:-0.15px;}
#t2p_25{left:206px;bottom:200px;}
#t2q_25{left:226px;bottom:201px;letter-spacing:-0.14px;word-spacing:0.02px;}
#t2r_25{left:226px;bottom:188px;letter-spacing:-0.15px;}
#t2s_25{left:206px;bottom:174px;letter-spacing:-0.13px;word-spacing:0.11px;}
#t2t_25{left:261px;bottom:160px;}
#t2u_25{left:281px;bottom:160px;letter-spacing:-0.13px;word-spacing:-0.01px;}
#t2v_25{left:261px;bottom:146px;}
#t2w_25{left:281px;bottom:146px;letter-spacing:-0.14px;word-spacing:0.07px;}
#t2x_25{left:261px;bottom:132px;}
#t2y_25{left:281px;bottom:133px;letter-spacing:-0.19px;}
#t2z_25{left:206px;bottom:119px;}
#t30_25{left:226px;bottom:119px;letter-spacing:-0.14px;}
#t31_25{left:206px;bottom:105px;}
#t32_25{left:226px;bottom:105px;letter-spacing:-0.14px;word-spacing:0.08px;}
#t33_25{left:1233px;bottom:197px;letter-spacing:-0.02px;}
#t34_25{left:1227px;bottom:120px;letter-spacing:-0.05px;}
#t35_25{left:1233px;bottom:108px;letter-spacing:0.03px;}
#t36_25{left:1219px;bottom:564px;letter-spacing:-0.15px;}
#t37_25{left:1229px;bottom:361px;letter-spacing:-0.16px;}
#t38_25{left:1223px;bottom:240px;letter-spacing:-0.13px;word-spacing:-0.03px;}
#t39_25{left:99px;bottom:24px;letter-spacing:0.1px;}

.s0_25{font-size:30px;font-family:DejaVuSans_7l;color:#000;}
.s1_25{font-size:17px;font-family:Carlito-Bold_8j;color:#000;}
.s2_25{font-size:18px;font-family:Carlito_7-;color:#000;}
.s3_25{font-size:18px;font-family:LiberationSans_7g;color:#000;}
.s4_25{font-size:15px;font-family:Carlito-Bold_8j;color:#000;}
.s5_25{font-size:14px;font-family:Carlito-Bold_8j;color:#000;}
.s6_25{font-size:14px;font-family:Carlito_7-;color:#000;}
.s7_25{font-size:14px;font-family:Carlito-Italic_84;color:#000;}
.s8_25{font-size:14px;font-family:LiberationSans_7g;color:#000;}
.s9_25{font-size:10px;font-family:Carlito_7-;color:#000;}
.sa_25{font-size:8px;font-family:Carlito_7-;color:#000;}
.sb_25{font-size:9px;font-family:Carlito_7-;color:#000;}
.sc_25{font-size:8px;font-family:Carlito-Bold_8j;color:#000;}
.sd_25{font-size:14px;font-family:LiberationMono_7b;color:#000;}
.se_25{font-size:14px;font-family:OpenSymbol_8e;color:#000;}
.sf_25{font-size:10px;font-family:Carlito-Bold_8j;color:#000;}
.sg_25{font-size:15px;font-family:Carlito_7-;color:#0563C1;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts25" type="text/css" >

@font-face {
	font-family: Carlito-Bold_8j;
	src: url("fonts/Carlito-Bold_8j.woff") format("woff");
}

@font-face {
	font-family: Carlito-Italic_84;
	src: url("fonts/Carlito-Italic_84.woff") format("woff");
}

@font-face {
	font-family: Carlito_7-;
	src: url("fonts/Carlito_7-.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_7l;
	src: url("fonts/DejaVuSans_7l.woff") format("woff");
}

@font-face {
	font-family: LiberationMono_7b;
	src: url("fonts/LiberationMono_7b.woff") format("woff");
}

@font-face {
	font-family: LiberationSans_7g;
	src: url("fonts/LiberationSans_7g.woff") format("woff");
}

@font-face {
	font-family: OpenSymbol_8e;
	src: url("fonts/OpenSymbol_8e.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg25Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg25" style="-webkit-user-select: none;"><object width="1466" height="825" data="25/25.svg" type="image/svg+xml" id="pdf25" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_25" class="t s0_25">Generative AI Project Life Cycle </span><span id="t2_25" class="t s0_25">- Optimize and deploy model for inference </span>
<span id="t3_25" class="t s1_25" data-mappings='[[7,"ti"],[18,"ti"]]'>Applicaon Integraon </span>
<span id="t4_25" class="t s1_25" data-mappings='[[2,"ti"],[7,"ti"]]'>Opmizaon </span>
<span id="t5_25" class="t s2_25" data-mappings='[[75,"ti"]]'>The things that we have to consider to integrate the LLM model into applicaons. </span>
<span id="t6_25" class="t s3_25">• </span><span id="t7_25" class="t s2_25" data-mappings='[[22,"ti"]]'>How your LLM will funcon in deployment ? </span>
<span id="t8_25" class="t s3_25">• </span><span id="t9_25" class="t s2_25" data-mappings='[[50,"ti"]]'>How fast do you need your model to generate compleons? </span>
<span id="ta_25" class="t s3_25">• </span><span id="tb_25" class="t s2_25">What compute budget do you have available? </span>
<span id="tc_25" class="t s3_25">• </span><span id="td_25" class="t s2_25" data-mappings='[[26,"ff"]]'>Are you willing to trade oﬀ model performance for improved inference speed or lower storage? </span>
<span id="te_25" class="t s4_25">Overview of </span><span id="tf_25" class="t s4_25" data-mappings='[[2,"ti"],[7,"ti"]]'>Opmisaon techniques for LLM </span>
<span id="tg_25" class="t s4_25">Techniques </span><span id="th_25" class="t s4_25">Details </span>
<span id="ti_25" class="t s5_25" data-mappings='[[3,"ti"],[7,"ti"]]'>Disllaon </span><span id="tj_25" class="t s6_25" data-mappings='[[9,"ti"],[13,"ti"]]'>Model Disllaon is a technique that focuses on using </span><span id="tk_25" class="t s6_25">a larger (teacher </span><span id="tl_25" class="t s6_25">) model to train a smaller model (</span><span id="tm_25" class="t s7_25">student </span><span id="tn_25" class="t s6_25">). You then use the smaller model for </span>
<span id="to_25" class="t s6_25">inference to lower your storage and compute budget. </span>
<span id="tp_25" class="t s8_25">• </span><span id="tq_25" class="t s6_25">You freeze the teacher model's weights </span>
<span id="tr_25" class="t s8_25">• </span><span id="ts_25" class="t s6_25" data-mappings='[[29,"ti"]]'>and use it to generate compleons for your training data </span>
<span id="tt_25" class="t s8_25">• </span><span id="tu_25" class="t s6_25" data-mappings='[[12,"ti"],[36,"ti"]]'>At the same me, you generate compleons for the training data using your student model </span>
<span id="tv_25" class="t s8_25">• </span><span id="tw_25" class="t s6_25" data-mappings='[[17,"ti"],[21,"ti"],[96,"ti"],[114,"ti"],[118,"ti"]]'>The knowledge disllaon between teacher and student model is achieved by minimizing a loss funcon called the disllaon loss </span>
<span id="tx_25" class="t s9_25">LLM </span>
<span id="ty_25" class="t s9_25">teacher </span>
<span id="tz_25" class="t sa_25">LLM </span>
<span id="t10_25" class="t sa_25">student </span>
<span id="t11_25" class="t s9_25" data-mappings='[[2,"ft"]]'>Soﬅmax </span>
<span id="t12_25" class="t sb_25">Knowledge </span>
<span id="t13_25" class="t sb_25" data-mappings='[[3,"ti"],[7,"ti"]]'>disllaon </span>
<span id="t14_25" class="t s9_25">labels </span>
<span id="t15_25" class="t s9_25" data-mappings='[[2,"ft"]]'>Soﬅmax </span>
<span id="t16_25" class="t s9_25">Training </span>
<span id="t17_25" class="t s9_25">dataset </span>
<span id="t18_25" class="t sb_25">Knowledge </span>
<span id="t19_25" class="t sb_25" data-mappings='[[3,"ti"],[7,"ti"]]'>disllaon </span>
<span id="t1a_25" class="t s9_25" data-mappings='[[6,"ti"]]'>predicons </span>
<span id="t1b_25" class="t s5_25" data-mappings='[[4,"ti"],[7,"ti"]]'>Quansaon </span><span id="t1c_25" class="t s6_25" data-mappings='[[19,"ft"]]'>In this technique aﬅer model is trained you can perform </span><span id="t1d_25" class="t s5_25">P</span><span id="t1e_25" class="t s6_25">ost-</span><span id="t1f_25" class="t s5_25">T</span><span id="t1g_25" class="t s6_25">raining </span><span id="t1h_25" class="t s5_25">Q</span><span id="t1i_25" class="t s6_25" data-mappings='[[3,"ti"],[6,"ti"]]'>uanzaon (</span><span id="t1j_25" class="t s5_25">PTQ</span><span id="t1k_25" class="t s6_25">) for deployment. </span>
<span id="t1l_25" class="t s8_25">• </span><span id="t1m_25" class="t s6_25" data-mappings='[[64,"ti"],[84,"fl"],[87,"ti"]]'>PTQ transforms a model's weights to a lower precision representaon, such as 16-bit ﬂoang point or 8-bit integer. This reduces the model size and </span>
<span id="t1n_25" class="t s6_25">memory footprint, as well as the compute resources needed for model serving </span>
<span id="t1o_25" class="t s8_25">• </span><span id="t1p_25" class="t s6_25" data-mappings='[[17,"ff"],[32,"ti"],[41,"ti"],[44,"ti"],[83,"ti"],[102,"ti"],[134,"ti"]]'>There are trade-oﬀs because somemes quanzaon results in a small percentage reducon in model evaluaon metrics. However, that reducon </span>
<span id="t1q_25" class="t s6_25" data-mappings='[[5,"ft"]]'>can oﬅen be worth the cost savings and performance gains. </span>
<span id="t1r_25" class="t sa_25">Bits </span><span id="t1s_25" class="t sa_25">Exponents </span><span id="t1t_25" class="t sa_25" data-mappings='[[4,"ti"]]'>Fracon </span>
<span id="t1u_25" class="t sa_25">Memory </span>
<span id="t1v_25" class="t sa_25">(to store 1 value) </span>
<span id="t1w_25" class="t sa_25">Store value of PI </span>
<span id="t1x_25" class="t sa_25">(example) </span>
<span id="t1y_25" class="t sc_25">FP32 </span><span id="t1z_25" class="t sa_25">32 </span><span id="t20_25" class="t sa_25">8 </span><span id="t21_25" class="t sa_25">23 </span><span id="t22_25" class="t sa_25">4 bytes </span><span id="t23_25" class="t sa_25">3.1415920257568359375 </span>
<span id="t24_25" class="t sc_25">FP16 </span><span id="t25_25" class="t sa_25">16 </span><span id="t26_25" class="t sa_25">5 </span><span id="t27_25" class="t sa_25">10 </span><span id="t28_25" class="t sa_25">2 bytes </span><span id="t29_25" class="t sa_25">3.140625 </span>
<span id="t2a_25" class="t sc_25">BFLOAT16 </span><span id="t2b_25" class="t sa_25">16 </span><span id="t2c_25" class="t sa_25">8 </span><span id="t2d_25" class="t sa_25">7 </span><span id="t2e_25" class="t sa_25">2 bytes </span><span id="t2f_25" class="t sa_25">3.140625 </span>
<span id="t2g_25" class="t sc_25">INT8 </span><span id="t2h_25" class="t sa_25">8 </span><span id="t2i_25" class="t sa_25">-/- </span><span id="t2j_25" class="t sa_25">7 </span><span id="t2k_25" class="t sa_25">1 byte </span>
<span id="t2l_25" class="t s5_25">Model </span>
<span id="t2m_25" class="t s5_25">Pruning </span>
<span id="t2n_25" class="t s6_25" data-mappings='[[68,"tt"]]'>Model Pruning, removes redundant model parameters that contribute lile to the model's performance i.e. removes model weights with values close </span>
<span id="t2o_25" class="t s6_25">or equal to zero </span>
<span id="t2p_25" class="t s8_25">• </span><span id="t2q_25" class="t s6_25" data-mappings='[[74,"ti"],[107,"ti"]]'>The goal of model pruning is to reduce model size for inference by eliminang weights that are not contribung much to overall model </span>
<span id="t2r_25" class="t s6_25">performance. </span>
<span id="t2s_25" class="t s6_25">Pruning methods </span>
<span id="t2t_25" class="t sd_25">o </span><span id="t2u_25" class="t s6_25">Full model re-training </span>
<span id="t2v_25" class="t sd_25">o </span><span id="t2w_25" class="t s6_25" data-mappings='[[11,"ffi"]]'>Parameter Eﬃcient Fine tuning (PEFT) such as LoRA </span>
<span id="t2x_25" class="t sd_25">o </span><span id="t2y_25" class="t s6_25">Post-training </span>
<span id="t2z_25" class="t se_25"> </span><span id="t30_25" class="t s6_25">In theory pruning reduces model size and improves performance </span>
<span id="t31_25" class="t se_25"> </span><span id="t32_25" class="t s6_25" data-mappings='[[7,"ti"]]'>In pracce, only small percentage (%) in LLM are zero-weights </span>
<span id="t33_25" class="t s9_25">LLM </span>
<span id="t34_25" class="t sf_25">Pruned </span>
<span id="t35_25" class="t sf_25">LLM </span>
<span id="t36_25" class="t s6_25" data-mappings='[[3,"ti"],[7,"ti"]]'>Disllaon </span>
<span id="t37_25" class="t s6_25" data-mappings='[[4,"ti"],[7,"ti"]]'>Quansaon </span>
<span id="t38_25" class="t s6_25">Model Pruning </span>
<span id="t39_25" class="t sg_25">Kumar </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
