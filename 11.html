<!DOCTYPE html>
<html>
<head>
<meta http-equiv="X-UA-Compatible" content="IE=Edge" />
<meta charset="utf-8" />
</head>

<body style="margin: 0;">

<div id="p11" style="overflow: hidden; position: relative; background-color: white; width: 1466px; height: 825px;">

<!-- Begin shared CSS values -->
<style class="shared-css" type="text/css" >
.t {
	transform-origin: bottom left;
	z-index: 2;
	position: absolute;
	white-space: pre;
	overflow: visible;
	line-height: 1.5;
}
.text-container {
	white-space: pre;
}
@supports (-webkit-touch-callout: none) {
	.text-container {
		white-space: normal;
	}
}
</style>
<!-- End shared CSS values -->


<!-- Begin inline CSS -->
<style type="text/css" >

#t1_11{left:73px;bottom:747px;letter-spacing:-0.16px;word-spacing:0.28px;}
#t2_11{left:75px;bottom:711px;letter-spacing:0.1px;word-spacing:1.85px;}
#t3_11{left:75px;bottom:688px;letter-spacing:0.16px;word-spacing:0.05px;}
#t4_11{left:71px;bottom:621px;letter-spacing:-0.1px;word-spacing:0.06px;}
#t5_11{left:441px;bottom:621px;}
#t6_11{left:478px;bottom:621px;letter-spacing:-0.07px;word-spacing:0.06px;}
#t7_11{left:664px;bottom:621px;letter-spacing:-0.09px;word-spacing:0.04px;}
#t8_11{left:885px;bottom:621px;letter-spacing:-0.13px;word-spacing:0.12px;}
#t9_11{left:71px;bottom:600px;letter-spacing:-0.08px;}
#ta_11{left:222px;bottom:600px;letter-spacing:-0.09px;}
#tb_11{left:288px;bottom:600px;letter-spacing:-0.14px;word-spacing:0.05px;}
#tc_11{left:478px;bottom:600px;}
#td_11{left:1097px;bottom:606px;letter-spacing:0.06px;}
#te_11{left:1115px;bottom:606px;letter-spacing:-0.1px;}
#tf_11{left:1150px;bottom:606px;}
#tg_11{left:1166px;bottom:606px;letter-spacing:-0.07px;}
#th_11{left:1210px;bottom:606px;letter-spacing:-0.17px;}
#ti_11{left:1242px;bottom:606px;letter-spacing:-0.37px;}
#tj_11{left:1100px;bottom:515px;letter-spacing:0.06px;}
#tk_11{left:1118px;bottom:515px;letter-spacing:-0.17px;}
#tl_11{left:1153px;bottom:515px;}
#tm_11{left:1169px;bottom:515px;letter-spacing:-0.1px;}
#tn_11{left:1213px;bottom:515px;letter-spacing:-0.17px;}
#to_11{left:1246px;bottom:515px;letter-spacing:-0.22px;}
#tp_11{left:1286px;bottom:514px;}
#tq_11{left:70px;bottom:647px;letter-spacing:-0.17px;word-spacing:0.09px;}
#tr_11{left:72px;bottom:561px;letter-spacing:-0.15px;}
#ts_11{left:88px;bottom:561px;letter-spacing:-0.12px;word-spacing:0.04px;}
#tt_11{left:264px;bottom:561px;letter-spacing:-0.07px;word-spacing:0.02px;}
#tu_11{left:72px;bottom:540px;letter-spacing:-0.14px;word-spacing:0.08px;}
#tv_11{left:72px;bottom:506px;letter-spacing:-0.09px;word-spacing:0.03px;}
#tw_11{left:72px;bottom:484px;letter-spacing:-0.08px;word-spacing:0.03px;}
#tx_11{left:634px;bottom:484px;letter-spacing:-0.11px;word-spacing:0.1px;}
#ty_11{left:1058px;bottom:261px;letter-spacing:-0.2px;}
#tz_11{left:1138px;bottom:282px;letter-spacing:-0.2px;}
#t10_11{left:1138px;bottom:389px;letter-spacing:-0.25px;}
#t11_11{left:1141px;bottom:376px;letter-spacing:-0.18px;}
#t12_11{left:1056px;bottom:189px;letter-spacing:0.21px;}
#t13_11{left:1137px;bottom:189px;letter-spacing:0.21px;}
#t14_11{left:895px;bottom:68px;letter-spacing:-0.15px;}
#t15_11{left:901px;bottom:152px;letter-spacing:-0.33px;}
#t16_11{left:1137px;bottom:448px;letter-spacing:-0.18px;}
#t17_11{left:962px;bottom:253px;letter-spacing:0.07px;}
#t18_11{left:962px;bottom:242px;letter-spacing:0.03px;}
#t19_11{left:1242px;bottom:404px;letter-spacing:0.04px;}
#t1a_11{left:1269px;bottom:404px;letter-spacing:0.04px;}
#t1b_11{left:1296px;bottom:405px;letter-spacing:0.04px;}
#t1c_11{left:1324px;bottom:403px;letter-spacing:0.11px;}
#t1d_11{left:931px;bottom:76px;letter-spacing:-0.18px;word-spacing:0.06px;}
#t1e_11{left:1157px;bottom:76px;letter-spacing:-0.2px;word-spacing:0.05px;}
#t1f_11{left:931px;bottom:64px;letter-spacing:-0.2px;word-spacing:0.04px;}
#t1g_11{left:931px;bottom:51px;letter-spacing:-0.2px;word-spacing:0.04px;}
#t1h_11{left:931px;bottom:38px;letter-spacing:-0.19px;}
#t1i_11{left:947px;bottom:114px;letter-spacing:0.04px;}
#t1j_11{left:976px;bottom:114px;letter-spacing:0.04px;}
#t1k_11{left:1014px;bottom:114px;letter-spacing:0.04px;}
#t1l_11{left:1047px;bottom:114px;letter-spacing:0.04px;}
#t1m_11{left:1202px;bottom:260px;letter-spacing:0.07px;}
#t1n_11{left:1203px;bottom:249px;letter-spacing:0.03px;}
#t1o_11{left:1282px;bottom:325px;letter-spacing:-0.26px;}
#t1p_11{left:1282px;bottom:312px;letter-spacing:-0.19px;}
#t1q_11{left:1282px;bottom:299px;letter-spacing:-0.27px;}
#t1r_11{left:1329px;bottom:299px;letter-spacing:-0.17px;word-spacing:-0.03px;}
#t1s_11{left:1282px;bottom:286px;letter-spacing:-0.21px;}
#t1t_11{left:1282px;bottom:274px;}
#t1u_11{left:1316px;bottom:274px;letter-spacing:-0.18px;word-spacing:-0.01px;}
#t1v_11{left:1282px;bottom:261px;}
#t1w_11{left:1316px;bottom:261px;letter-spacing:-0.22px;word-spacing:0.04px;}
#t1x_11{left:1282px;bottom:249px;}
#t1y_11{left:1316px;bottom:248px;letter-spacing:-0.2px;}
#t1z_11{left:1282px;bottom:236px;}
#t20_11{left:1316px;bottom:235px;letter-spacing:-0.28px;}
#t21_11{left:1282px;bottom:223px;}
#t22_11{left:1316px;bottom:222px;letter-spacing:-0.22px;}
#t23_11{left:1282px;bottom:210px;}
#t24_11{left:1316px;bottom:210px;letter-spacing:-0.75px;}
#t25_11{left:71px;bottom:447px;letter-spacing:-0.12px;word-spacing:0.03px;}
#t26_11{left:73px;bottom:422px;letter-spacing:-0.08px;word-spacing:0.06px;}
#t27_11{left:507px;bottom:422px;letter-spacing:-0.14px;}
#t28_11{left:568px;bottom:422px;letter-spacing:-0.12px;}
#t29_11{left:597px;bottom:422px;letter-spacing:-0.14px;}
#t2a_11{left:654px;bottom:422px;letter-spacing:-0.08px;word-spacing:-0.01px;}
#t2b_11{left:73px;bottom:400px;letter-spacing:-0.11px;word-spacing:0.06px;}
#t2c_11{left:73px;bottom:380px;}
#t2d_11{left:94px;bottom:379px;letter-spacing:-0.1px;word-spacing:0.05px;}
#t2e_11{left:94px;bottom:357px;letter-spacing:-0.11px;word-spacing:0.06px;}
#t2f_11{left:73px;bottom:324px;}
#t2g_11{left:94px;bottom:324px;letter-spacing:-0.09px;word-spacing:0.06px;}
#t2h_11{left:94px;bottom:302px;letter-spacing:-0.1px;word-spacing:0.04px;}
#t2i_11{left:94px;bottom:280px;letter-spacing:-0.09px;word-spacing:0.07px;}
#t2j_11{left:75px;bottom:248px;letter-spacing:-0.11px;word-spacing:0.02px;}
#t2k_11{left:229px;bottom:248px;letter-spacing:-0.06px;}
#t2l_11{left:75px;bottom:226px;letter-spacing:-0.08px;word-spacing:0.03px;}
#t2m_11{left:75px;bottom:205px;letter-spacing:-0.1px;word-spacing:0.04px;}
#t2n_11{left:75px;bottom:183px;letter-spacing:-0.05px;word-spacing:-0.04px;}
#t2o_11{left:73px;bottom:157px;letter-spacing:-0.11px;word-spacing:0.02px;}
#t2p_11{left:247px;bottom:157px;letter-spacing:-0.08px;}
#t2q_11{left:372px;bottom:157px;letter-spacing:-0.08px;}
#t2r_11{left:73px;bottom:135px;letter-spacing:-0.08px;word-spacing:0.02px;}
#t2s_11{left:73px;bottom:114px;letter-spacing:-0.13px;word-spacing:0.07px;}
#t2t_11{left:74px;bottom:81px;letter-spacing:-0.11px;word-spacing:0.02px;}
#t2u_11{left:226px;bottom:81px;letter-spacing:-0.11px;word-spacing:0.07px;}
#t2v_11{left:74px;bottom:60px;letter-spacing:-0.1px;word-spacing:0.06px;}
#t2w_11{left:74px;bottom:38px;letter-spacing:-0.12px;word-spacing:0.06px;}
#t2x_11{left:117px;bottom:2px;letter-spacing:0.1px;}
#t2y_11{left:1314px;bottom:565px;letter-spacing:-0.22px;word-spacing:0.03px;}
#t2z_11{left:1314px;bottom:553px;letter-spacing:-0.18px;word-spacing:0.01px;}
#t30_11{left:1318px;bottom:540px;letter-spacing:-0.18px;word-spacing:0.08px;}
#t31_11{left:1376px;bottom:540px;letter-spacing:-0.21px;}
#t32_11{left:1403px;bottom:540px;letter-spacing:-0.06px;}
#t33_11{left:1312px;bottom:527px;letter-spacing:-0.18px;word-spacing:-0.07px;}
#t34_11{left:1317px;bottom:514px;letter-spacing:-0.2px;}
#t35_11{left:1314px;bottom:501px;letter-spacing:-0.23px;word-spacing:0.13px;}
#t36_11{left:1355px;bottom:501px;letter-spacing:-0.27px;}
#t37_11{left:1380px;bottom:501px;letter-spacing:-0.14px;}
#t38_11{left:1399px;bottom:501px;letter-spacing:-0.24px;}

.s0_11{font-size:38px;font-family:DejaVuSans_6t;color:#000;}
.s1_11{font-size:18px;font-family:Carlito_77;color:#000;}
.s2_11{font-size:17px;font-family:Carlito_77;color:#333;}
.s3_11{font-size:17px;font-family:Carlito-Bold_7r;color:#333;}
.s4_11{font-size:17px;font-family:Carlito_77;color:#1F1F1F;}
.s5_11{font-size:17px;font-family:Carlito-Italic_7c;color:#333;}
.s6_11{font-size:17px;font-family:Carlito-Italic_7c;color:#0563C1;}
.s7_11{font-size:17px;font-family:Carlito-Bold_7r;color:#000;}
.s8_11{font-size:18px;font-family:Carlito-Bold_7r;color:#000;}
.s9_11{font-size:11px;font-family:Carlito-Bold_7r;color:#000;}
.sa_11{font-size:8px;font-family:Carlito_77;color:#FFF;}
.sb_11{font-size:9px;font-family:Carlito_77;color:#000;}
.sc_11{font-size:9px;font-family:Carlito-Italic_7c;color:#000;}
.sd_11{font-size:11px;font-family:Carlito_77;color:#000;}
.se_11{font-size:11px;font-family:DejaVuSans-Bold_72;color:#374151;}
.sf_11{font-size:11px;font-family:DejaVuSans_6t;color:#374151;}
.sg_11{font-size:11px;font-family:LiberationSans_6o;color:#374151;}
.sh_11{font-size:17px;font-family:LiberationSans_6o;color:#333;}
.si_11{font-size:18px;font-family:Carlito_77;color:#8B8B8B;}
</style>
<!-- End inline CSS -->

<!-- Begin embedded font definitions -->
<style id="fonts11" type="text/css" >

@font-face {
	font-family: Carlito-Bold_7r;
	src: url("fonts/Carlito-Bold_7r.woff") format("woff");
}

@font-face {
	font-family: Carlito-Italic_7c;
	src: url("fonts/Carlito-Italic_7c.woff") format("woff");
}

@font-face {
	font-family: Carlito_77;
	src: url("fonts/Carlito_77.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans-Bold_72;
	src: url("fonts/DejaVuSans-Bold_72.woff") format("woff");
}

@font-face {
	font-family: DejaVuSans_6t;
	src: url("fonts/DejaVuSans_6t.woff") format("woff");
}

@font-face {
	font-family: LiberationSans_6o;
	src: url("fonts/LiberationSans_6o.woff") format("woff");
}

</style>
<!-- End embedded font definitions -->

<!-- Begin page background -->
<div id="pg11Overlay" style="width:100%; height:100%; position:absolute; z-index:1; background-color:rgba(0,0,0,0); -webkit-user-select: none;"></div>
<div id="pg11" style="-webkit-user-select: none;"><object width="1466" height="825" data="11/11.svg" type="image/svg+xml" id="pdf11" style="width:1466px; height:825px; -moz-transform:scale(1); z-index: 0;"></object></div>
<!-- End page background -->


<!-- Begin text definitions (Positioned/styled in CSS) -->
<div class="text-container"><span id="t1_11" class="t s0_11">High level working of Transformer </span>
<span id="t2_11" class="t s1_11" data-mappings='[[148,"tt"],[151,"ti"]]'>LLMs are boarder class of transformer models. The underlying transformer is a set of neural networks that consist of an encoder, decoder with self-aenon and </span>
<span id="t3_11" class="t s1_11" data-mappings='[[3,"ti"],[6,"tt"],[9,"ti"],[20,"ti"],[30,"tt"],[33,"ti"]]'>mul-aenon (or mul-headed aenon ) mechanism. </span>
<span id="t4_11" class="t s2_11" data-mappings='[[21,"ti"]]'>In the earlier generaon of recurrent neural network ( </span><span id="t5_11" class="t s3_11">RNN)</span><span id="t6_11" class="t s2_11">s , the algorithms learns to </span><span id="t7_11" class="t s4_11">each word next to its neighbour </span><span id="t8_11" class="t s2_11">as shown rather </span>
<span id="t9_11" class="t s2_11">than whole sentence. </span><span id="ta_11" class="t s5_11" data-mappings='[[3,"ti"]]'>(oponal </span><span id="tb_11" class="t s6_11" data-mappings='[[16,"ti"]]'>Next-Word-Predicon demo </span><span id="tc_11" class="t s5_11">) </span>
<span id="td_11" class="t s7_11">It </span><span id="te_11" class="t s7_11">was </span><span id="tf_11" class="t s7_11">a </span><span id="tg_11" class="t s7_11">great </span><span id="th_11" class="t s7_11">day </span><span id="ti_11" class="t s7_11">today. </span>
<span id="tj_11" class="t s7_11">It </span><span id="tk_11" class="t s7_11">was </span><span id="tl_11" class="t s7_11">a </span><span id="tm_11" class="t s7_11">great </span><span id="tn_11" class="t s7_11">day </span><span id="to_11" class="t s7_11">today</span><span id="tp_11" class="t s8_11">. </span>
<span id="tq_11" class="t s3_11">Transformer’s architecture </span>
<span id="tr_11" class="t s2_11">In </span><span id="ts_11" class="t s3_11">transformer architecture </span><span id="tt_11" class="t s2_11">the power lies in its ability to learn the relevance and context of all of the words in a sentence as shown. </span>
<span id="tu_11" class="t s2_11" data-mappings='[[6,"ti"],[42,"tt"],[45,"ti"]]'>Generave Pre-Trained Transformers (GPT) aenon weights are calculated between each word and every other word. </span>
<span id="tv_11" class="t s2_11" data-mappings='[[21,"tt"],[24,"ti"],[49,"ti"]]'>Transformer applies aenon weights to those relaonships so that the model learns the relevance of each word to each other words </span>
<span id="tw_11" class="t s2_11" data-mappings='[[5,"tt"]]'>no maer where they are in the input. This gives the algorithm the ability to learn </span><span id="tx_11" class="t s4_11">wider context of the document/sentence/prompt. </span>
<span id="ty_11" class="t s9_11">Encoder </span>
<span id="tz_11" class="t s9_11">Decoder </span>
<span id="t10_11" class="t s9_11" data-mappings='[[2,"ft"]]'>Soﬅmax </span>
<span id="t11_11" class="t s9_11">output </span>
<span id="t12_11" class="t sa_11">Embedding </span><span id="t13_11" class="t sa_11">Embedding </span>
<span id="t14_11" class="t s9_11">input: </span>
<span id="t15_11" class="t s9_11">Tokenizer </span>
<span id="t16_11" class="t s9_11">output </span>
<span id="t17_11" class="t sb_11" data-mappings='[[3,"ti"]]'>Mul-headed </span>
<span id="t18_11" class="t sb_11" data-mappings='[[6,"tt"],[9,"ti"]]'>self-aenon </span>
<span id="t19_11" class="t sb_11">914 </span><span id="t1a_11" class="t sb_11">858 </span><span id="t1b_11" class="t sb_11">592 </span>
<span id="t1c_11" class="t sc_11">nnn </span>
<span id="t1d_11" class="t sd_11" data-mappings='[[4,"ti"]]'>idenfy all of the companies name, date and places </span><span id="t1e_11" class="t sd_11" data-mappings='[[33,"fi"]]'>in "so we started NetSafe to redeﬁne Cloud, </span>
<span id="t1f_11" class="t sd_11">Network and Data Security. Since 2019, we have built the market-leading cloud security company </span>
<span id="t1g_11" class="t sd_11" data-mappings='[[77,"ffi"]]'>and an award-winning culture powered by hundreds of employees spread across oﬃces in Santa </span>
<span id="t1h_11" class="t sd_11">Clara, St. Louis, Bangalore, London, Melbourne, and Tokyo." </span>
<span id="t1i_11" class="t sb_11">419 </span><span id="t1j_11" class="t sb_11">683 </span><span id="t1k_11" class="t sb_11">295 </span><span id="t1l_11" class="t sb_11">828 </span>
<span id="t1m_11" class="t sb_11" data-mappings='[[3,"ti"]]'>Mul-headed </span>
<span id="t1n_11" class="t sb_11" data-mappings='[[6,"tt"],[9,"ti"]]'>self-aenon </span>
<span id="t1o_11" class="t se_11">1.Company Name: </span>
<span id="t1p_11" class="t sf_11">NetSafe </span>
<span id="t1q_11" class="t se_11">2.Date: </span><span id="t1r_11" class="t sf_11">Since 2019 </span>
<span id="t1s_11" class="t se_11">3.Places: </span>
<span id="t1t_11" class="t sg_11">• </span><span id="t1u_11" class="t sf_11">Santa Clara </span>
<span id="t1v_11" class="t sg_11">• </span><span id="t1w_11" class="t sf_11">St. Louis </span>
<span id="t1x_11" class="t sg_11">• </span><span id="t1y_11" class="t sf_11">Bangalore </span>
<span id="t1z_11" class="t sg_11">• </span><span id="t20_11" class="t sf_11">London </span>
<span id="t21_11" class="t sg_11">• </span><span id="t22_11" class="t sf_11">Melbourne </span>
<span id="t23_11" class="t sg_11">• </span><span id="t24_11" class="t sf_11">Tokyo </span>
<span id="t25_11" class="t s3_11">High level working of Transformer </span>
<span id="t26_11" class="t s2_11" data-mappings='[[50,"ti"]]'>The transformer architecture is split into two disnct parts, the </span><span id="t27_11" class="t s3_11">encoder </span><span id="t28_11" class="t s2_11">and </span><span id="t29_11" class="t s3_11">decoder</span><span id="t2a_11" class="t s2_11" data-mappings='[[34,"ti"]]'>. These components work in conjuncon </span>
<span id="t2b_11" class="t s2_11" data-mappings='[[52,"ti"]]'>with each other, and they share a number of similaries. </span>
<span id="t2c_11" class="t sh_11">• </span><span id="t2d_11" class="t s2_11" data-mappings='[[26,"ti"],[28,"ti"]]'>ML models are just big stascal calculators, and they work with numbers. So before passing texts into the model to </span>
<span id="t2e_11" class="t s2_11" data-mappings='[[18,"fi"]]'>process, you must ﬁrst tokenize the words. </span>
<span id="t2f_11" class="t sh_11">• </span><span id="t2g_11" class="t s2_11" data-mappings='[[58,"ti"]]'>The encoder encodes input sequences into a deep representaon of the structure and meaning of the input.  The decoder, </span>
<span id="t2h_11" class="t s2_11">working from input token triggers, uses the encoder's contextual understanding to generate new tokens. It does this in a </span>
<span id="t2i_11" class="t s2_11" data-mappings='[[7,"ti"],[25,"ti"]]'>loop unl some stop condion has been reached. </span>
<span id="t2j_11" class="t s3_11">Encoder-only models </span><span id="t2k_11" class="t s2_11">work as sequence-to-sequence models, the input sequence and the output sequence or the </span>
<span id="t2l_11" class="t s2_11" data-mappings='[[68,"ti"]]'>same length. Their use is less common these days, but by adding addional layers to the architecture, you can train </span>
<span id="t2m_11" class="t s2_11" data-mappings='[[37,"fi"],[40,"ti"],[61,"ti"]]'>encoder-only models to perform classiﬁcaon tasks such as senment analysis but is an example of an encoder- </span>
<span id="t2n_11" class="t s2_11">only model. </span>
<span id="t2o_11" class="t s3_11">Encoder-decoder models</span><span id="t2p_11" class="t s2_11">, perform well on </span><span id="t2q_11" class="t s2_11" data-mappings='[[42,"ti"]]'>sequence-to-sequence tasks such as translaon, where the input </span>
<span id="t2r_11" class="t s2_11" data-mappings='[[42,"ff"]]'>sequence and the output sequence can be diﬀerent lengths. You can also scale and train this type of model to </span>
<span id="t2s_11" class="t s2_11" data-mappings='[[27,"ti"]]'>perform general text generaon tasks example BART and T5 </span>
<span id="t2t_11" class="t s3_11">Decoder-only models </span><span id="t2u_11" class="t s2_11" data-mappings='[[56,"ti"]]'>are some of the most commonly used today. Their capabilies have grown. These models can </span>
<span id="t2v_11" class="t s2_11">now generalize to most tasks. Popular decoder-only models include the GPT family of models, BLOOM, Jurassic, </span>
<span id="t2w_11" class="t s2_11">LLaMA, CTRL, Transformer XL and many more. </span>
<span id="t2x_11" class="t si_11">Kumar </span>
<span id="t2y_11" class="t sd_11" data-mappings='[[1,"tt"],[4,"ti"]]'>Aenon map weights. </span>
<span id="t2z_11" class="t sd_11">In this example you can </span>
<span id="t30_11" class="t sd_11">see the work </span><span id="t31_11" class="t s9_11">today </span><span id="t32_11" class="t sd_11">is </span>
<span id="t33_11" class="t sd_11">strongly connected with </span>
<span id="t34_11" class="t sd_11" data-mappings='[[11,"tt"],[14,"ti"]]'>or paying aenon to </span>
<span id="t35_11" class="t sd_11">the word </span><span id="t36_11" class="t s9_11">great </span><span id="t37_11" class="t sd_11">and </span><span id="t38_11" class="t s9_11">day </span></div>
<!-- End text definitions -->


</div>
</body>
</html>
